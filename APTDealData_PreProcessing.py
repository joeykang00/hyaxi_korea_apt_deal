# pip install pandas matplotlib glob PIL

import os
import re
import platform
import zipfile

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import matplotlib.font_manager as fm

from glob import glob
from PIL import Image


# ======================================================================
# ê¸°ë³¸ ì„¤ì •
# ======================================================================

DATA_DIR = "./data/"
PREPROCESSED_DIR = "./preprocessed/"

pd.set_option("display.max_columns", None)
pd.set_option("display.max_colwidth", None)
pd.set_option("display.width", None)

def prepare_csv_from_zip(data_dir: str, csv_filename: str, zip_filename: str) -> str:
    csv_path = os.path.join(data_dir, csv_filename)
    zip_path = os.path.join(data_dir, zip_filename)

    if not os.path.exists(csv_path):
        print(f"'{csv_path}' is not exist. Checking Zip file.")
        if os.path.exists(zip_path):
            print(f"'{zip_path}' is found. Unzip...")
            try:
                with zipfile.ZipFile(zip_path, "r") as zip_ref:
                    zip_ref.extractall(data_dir)
                print(f"Completed Unzip. '{csv_path}' will be used.")
            except Exception as e:
                print(f"Unzip error: {e}")
                exit()
        else:
            print(f"Error: '{csv_filename}' and '{zip_filename}' are not found.")
            exit()
    return csv_path


def create_split_zip(source_file, output_prefix, chunk_size_mb=100):

    temp_zip = "temp_zip_file.zip"

    print(f"creating temp zip file... ({temp_zip})")
    with zipfile.ZipFile(temp_zip, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:
        zipf.write(source_file, arcname=os.path.basename(source_file))

    print(f"{chunk_size_mb:.1f}MB split zip...")

    part_num = 1
    with open(temp_zip, 'rb') as f:
        while True:
            chunk = f.read(chunk_size_mb*1024*1024)
            if not chunk:
                break

            part_filename = f"{output_prefix}.{part_num:03d}"

            with open(part_filename, 'wb') as chunk_file:
                chunk_file.write(chunk)

            print(f"created   -> {part_filename}")
            part_num += 1

    # 3. ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.remove(temp_zip)


# í‘œì¤€ ì—´ ì´ë¦„
COLUMNS_STANDARD = [
    "ë‚ ì§œ", "ê°•ì›", "ê²½ê¸°", "ê²½ë‚¨", "ê²½ë¶", "ê´‘ì£¼", "ëŒ€êµ¬", "ëŒ€ì „",
    "ë¶€ì‚°", "ì„œìš¸", "ì„¸ì¢…", "ìš¸ì‚°", "ì¸ì²œ", "ì „ë‚¨", "ì „ë¶", "ì œì£¼",
    "ì¶©ë‚¨", "ì¶©ë¶",
]

# ì‹œë„ëª… í‘œì¤€í™” ë§¤í•‘
RENAME_MAP = {
    "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°",
    "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ì„¸ì¢…",
    "ê²½ê¸°ë„": "ê²½ê¸°",
    "ê°•ì›ë„": "ê°•ì›",
    "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "ê°•ì›",
    "ì¶©ì²­ë¶ë„": "ì¶©ë¶",
    "ì¶©ì²­ë‚¨ë„": "ì¶©ë‚¨",
    "ì „ë¼ë¶ë„": "ì „ë¶",
    "ì „ë¶íŠ¹ë³„ìì¹˜ë„": "ì „ë¶",
    "ì „ë¼ë‚¨ë„": "ì „ë‚¨",
    "ê²½ìƒë¶ë„": "ê²½ë¶",
    "ê²½ìƒë‚¨ë„": "ê²½ë‚¨",
    "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "ì œì£¼",
    "ì œì£¼ë„": "ì œì£¼",
    "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸",
    "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°",
    "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬",
    "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ",
    "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼",
    "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „",
    "ì „êµ­": None,  # í•„ìš” ì—†ìœ¼ë©´ ì œê±°
    "ê±°ë˜ì¼": "ë‚ ì§œ",  # ì¼ë¶€ ë°ì´í„°ì—ì„œ 'ê±°ë˜ì¼' â†’ 'ë‚ ì§œ'
}


def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì‹œë„ëª…/ë‚ ì§œ ì—´ ì´ë¦„ì„ RENAME_MAP ê³¼ COLUMNS_STANDARD ê¸°ì¤€ìœ¼ë¡œ ë§ì¶°ì£¼ëŠ” í•¨ìˆ˜.
    """
    df = df.rename(columns=RENAME_MAP).drop(columns=[None], errors="ignore")
    df = df.reindex(columns=COLUMNS_STANDARD)
    return df


def find_file_by_pattern(data_dir: str, pattern: str) -> str:
    """
    data_dir ì•ˆì—ì„œ pattern(*.csv ë“±)ì— ë§¤ì¹­ë˜ëŠ” íŒŒì¼ì„ ì°¾ì•„ì„œ
    í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ ê²½ë¡œë¥¼ ë°˜í™˜.
    ì—†ê±°ë‚˜ ì—¬ëŸ¬ ê°œë©´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥ í›„ ì¢…ë£Œ.
    """
    search_pattern = os.path.join(data_dir, pattern)
    matches = glob(search_pattern)

    if len(matches) == 0:
        print(f"[Error] No file matched pattern: {search_pattern}")
        exit()
    elif len(matches) > 1:
        print(f"[Error] Multiple files matched pattern: {search_pattern}")
        for m in matches:
            print(" -", m)
        print("íŒ¨í„´ì— í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ë„ë¡ íŒŒì¼ëª…ì„ ì •ë¦¬í•´ ì£¼ì„¸ìš”.")
        exit()

    return matches[0]


def reduce_memory_usage(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameì˜ ì»¬ëŸ¼ë“¤ì„ ê°€ëŠ¥í•œ ì‘ì€ dtypeìœ¼ë¡œ ë‹¤ìš´ìºìŠ¤íŒ…í•´ì„œ
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì—¬ì¤Œ.
    """
    start_mem = df.memory_usage().sum() / 1024 ** 2

    for col in df.columns:
        col_type = df[col].dtype

        # ìˆ«ìí˜•ë§Œ ë‹¤ìš´ìºìŠ¤íŠ¸
        if str(col_type)[:3] == 'int':
            df[col] = pd.to_numeric(df[col], downcast='integer')
        elif str(col_type)[:5] == 'float':
            df[col] = pd.to_numeric(df[col], downcast='float')

    end_mem = df.memory_usage().sum() / 1024 ** 2
    print(f"Memory reduced: {start_mem:.3f} MB â†’ {end_mem:.3f} MB")

    return df


# ======================================================================
# 1) ê¸°ì¤€ê¸ˆë¦¬ ë°ì´í„°
# ======================================================================

def load_base_rate(data_dir: str) -> pd.DataFrame:
    # path = os.path.join(data_dir, "*_ê¸°ì¤€ê¸ˆë¦¬.csv")
    path = find_file_by_pattern(data_dir, "*_ê¸°ì¤€ê¸ˆë¦¬.csv")
    rate_csv = pd.read_csv(path, low_memory=False)

    rate_row = rate_csv.iloc[0, 4:]  # ì²« í–‰ì—ì„œ 5ë²ˆì§¸ ì—´ë¶€í„° ë‚ ì§œ êµ¬ê°„ë§Œ
    rate_df = rate_row.reset_index()
    rate_df.columns = ["date", "rate"]

    rate_df["date"] = pd.to_datetime(rate_df["date"])
    rate_df["rate"] = pd.to_numeric(rate_df["rate"], errors="coerce")

    rate_df = rate_df.rename(columns={"date": "ë‚ ì§œ", "rate": "ê¸°ì¤€ê¸ˆë¦¬"})

    print("ê¸°ì¤€ê¸ˆë¦¬ data")
    print(rate_df)

    return rate_df


# ======================================================================
# 2) ì¸êµ¬ìˆ˜ ë°ì´í„°
# ======================================================================

def load_population(data_dir: str) -> pd.DataFrame:
    # path = os.path.join(data_dir, "*_ì¸êµ¬ìˆ˜.csv")
    path = find_file_by_pattern(data_dir, "*_ì¸êµ¬ìˆ˜.csv")

    population_csv = pd.read_csv(
        path,
        encoding="cp949",
        header=[0, 1],
    )

    # ë©€í‹°ì»¬ëŸ¼ ì •ë¦¬
    population_csv.columns = pd.MultiIndex.from_tuples(
        [(str(a).strip(), str(b).strip()) for a, b in population_csv.columns]
    )

    # ì²« ì—´: í–‰ì •êµ¬ì—­
    region = population_csv.iloc[:, 0].astype(str).str.strip()

    # 'ì´ì¸êµ¬ìˆ˜' ì»¬ëŸ¼ë§Œ ì„ íƒ
    data = population_csv.iloc[:, 1:]
    mask_total = data.columns.get_level_values(1).str.contains("ì´ì¸êµ¬ìˆ˜")
    total_only = data.loc[:, mask_total]

    # í–‰ì •êµ¬ì—­ì„ ì¸ë±ìŠ¤ë¡œ
    total_only.index = region

    # ì—´ì˜ 1ë ˆë²¨(ë‚ ì§œ)ë§Œ ì‚¬ìš© â†’ datetime
    dates = total_only.columns.get_level_values(0)
    total_only.columns = pd.to_datetime(dates, format="%Y.%m", errors="coerce")

    # ì „ì¹˜ í›„ ë‚ ì§œë¥¼ ì²« ì»¬ëŸ¼ìœ¼ë¡œ
    population_df = total_only.T.reset_index().rename(columns={"index": "ë‚ ì§œ"})
    population_df.columns.name = "í–‰ì •êµ¬ì—­"

    # ì „êµ­ ì œê±°
    population_df.drop(columns=["ì „êµ­"], inplace=True)

    print("ì¸êµ¬ìˆ˜ data")
    print(population_df)

    return population_df


# ======================================================================
# 3) ì‹¤ì—…ë¥  ë°ì´í„°
# ======================================================================

def load_unemployment(data_dir: str) -> pd.DataFrame:
    # path = os.path.join(data_dir, "*_ì‹¤ì—…ë¥ .csv")
    path = find_file_by_pattern(data_dir, "*_ì‹¤ì—…ë¥ .csv")

    unemployment_csv = pd.read_csv(
        path,
        encoding="cp949",
        header=[0, 1],
    )

    # ë©€í‹°í—¤ë” ì •ë¦¬
    unemployment_csv.columns = pd.MultiIndex.from_tuples(
        [(str(a).strip(), str(b).strip()) for a, b in unemployment_csv.columns]
    )

    # ì²« ë‘ ì»¬ëŸ¼: [ì‹œë„/í–‰ì •êµ¬ì—­], [ì„±ë³„]
    region_col, gender_col = unemployment_csv.columns[0], unemployment_csv.columns[1]
    region = unemployment_csv[region_col].astype(str).str.strip()
    gender = unemployment_csv[gender_col].astype(str).str.strip()

    # ì„±ë³„ == 'ê³„' ë§Œ ì‚¬ìš©
    row_mask = gender == "ê³„"
    region = region[row_mask].replace({"ê³„": "ì „êµ­"})
    df_rows = unemployment_csv.loc[row_mask]

    # ë‚ ì§œ íŒ¨í„´ í•„í„° (YYYY.MM)
    date_pat = re.compile(r"^\d{4}\.\d{2}$")
    col_mask = [
        bool(date_pat.match(a)) for a in df_rows.columns.get_level_values(0)
    ]
    data = df_rows.loc[:, col_mask]

    # 1ë ˆë²¨(ë‚ ì§œ) ê¸°ì¤€ ê·¸ë£¹ â†’ ì²« ì»¬ëŸ¼(ë³´í†µ 'ê³„') ì‚¬ìš©
    data = data.T.groupby(level=0).first().T

    # í–‰ì •êµ¬ì—­ì„ ì¸ë±ìŠ¤ë¡œ, ì—´ì€ datetime
    data.index = region.values
    data.columns = pd.to_datetime(data.columns, format="%Y.%m")

    # ì „ì¹˜í•´ì„œ ë‚ ì§œë¥¼ ì²« ì»¬ëŸ¼ìœ¼ë¡œ
    unemployment_df = data.T.reset_index().rename(columns={"index": "ë‚ ì§œ"})

    # ìˆ«ìí˜• ë³€í™˜
    for c in unemployment_df.columns[1:]:
        unemployment_df[c] = pd.to_numeric(unemployment_df[c], errors="coerce")

    # 2017 ì´ì „ ì„¸ì¢…ì‹œ ì‹¤ì—…ë¥  0ìœ¼ë¡œ ì±„ìš°ê¸°
    if "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ" in unemployment_df.columns:
        unemployment_df["ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"] = unemployment_df["ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"].fillna(0)

    return unemployment_df


# ======================================================================
# 4) ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI)
# ======================================================================

def load_cpi(data_dir: str) -> pd.DataFrame:
    # path = os.path.join(data_dir, "*_ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv")
    path = find_file_by_pattern(data_dir, "*_ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv")
    price_csv = pd.read_csv(path, encoding="UTF-8-SIG")

    # 0í–‰, 1í–‰ + 5ë²ˆì§¸ ì—´ ì´í›„ (ë‚ ì§œ ì»¬ëŸ¼ë“¤ë§Œ)
    subset = price_csv.iloc[[0, 1], 5:]
    subset.index = ["ì´ì§€ìˆ˜", "ì „ë…„ë™ê¸°ëŒ€ë¹„ì¦ê°ë¥ "]

    cpi_df = subset.T.reset_index()
    cpi_df.columns = ["ë‚ ì§œ", "ì´ì§€ìˆ˜", "ì „ë…„ë™ê¸°ëŒ€ë¹„ì¦ê°ë¥ "]

    cpi_df["ì´ì§€ìˆ˜"] = pd.to_numeric(cpi_df["ì´ì§€ìˆ˜"], errors="coerce")
    cpi_df["ì „ë…„ë™ê¸°ëŒ€ë¹„ì¦ê°ë¥ "] = pd.to_numeric(
        cpi_df["ì „ë…„ë™ê¸°ëŒ€ë¹„ì¦ê°ë¥ "], errors="coerce"
    )

    cpi_df["ë‚ ì§œ"] = pd.to_datetime(
        cpi_df["ë‚ ì§œ"].astype(str),
        errors="coerce",
        format="%Y/%m",
    )

    cpi_df = cpi_df.rename(
        columns={
            "ì´ì§€ìˆ˜": "CPI_ì´ì§€ìˆ˜",
            "ì „ë…„ë™ê¸°ëŒ€ë¹„ì¦ê°ë¥ ": "CPI_ì „ë…„ë™ê¸°",
        }
    )

    print("ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ data")
    print(cpi_df)

    return cpi_df


# ======================================================================
# 5) ê°€ê³„ëŒ€ì¶œê¸ˆ ë°ì´í„°
# ======================================================================

def load_household_loan(data_dir: str) -> pd.DataFrame:
    # ì˜ˆ: ECOS_ê°€ê³„ëŒ€ì¶œ.csv ê°™ì€ ì´ë¦„
    path = find_file_by_pattern(data_dir, "*_ê°€ê³„ëŒ€ì¶œ.csv")

    df = pd.read_csv(path, encoding="UTF-8-SIG")

    # 'ì˜ˆê¸ˆì€í–‰'ë§Œ ì‚¬ìš©
    df = df[df["ê³„ì •í•­ëª©"].str.contains("ì˜ˆê¸ˆì€í–‰", na=False)]

    # ë‚ ì§œ ì»¬ëŸ¼ (ì•ìª½ 5ê°œ ì»¬ëŸ¼ ì´í›„ë¶€í„°ê°€ ë‚ ì§œ)
    date_cols = df.columns[5:]

    melted = df.melt(
        id_vars=["ì§€ì—­ì½”ë“œ"],
        value_vars=date_cols,
        var_name="ë‚ ì§œ",
        value_name="ëŒ€ì¶œê¸ˆì•¡",
    )

    # ë‚ ì§œ ë³€í™˜: "YYYY/MM" â†’ ì›”ì´ˆ ë‚ ì§œ
    melted["ë‚ ì§œ"] = pd.to_datetime(
        melted["ë‚ ì§œ"].astype(str),
        format="%Y/%m",
        errors="coerce",
    ) + pd.offsets.MonthBegin(0)

    # ìˆ«ìí˜• ë³€í™˜
    melted["ëŒ€ì¶œê¸ˆì•¡"] = pd.to_numeric(
        melted["ëŒ€ì¶œê¸ˆì•¡"].astype(str).str.replace(",", ""),
        errors="coerce",
    )

    print("ê°€ê³„ëŒ€ì¶œ data")
    print(melted)

    # ë‚ ì§œ = index, ì§€ì—­ì½”ë“œ = columns
    household_loan_df = (
        melted.pivot_table(
            index="ë‚ ì§œ",
            columns="ì§€ì—­ì½”ë“œ",
            values="ëŒ€ì¶œê¸ˆì•¡",
            aggfunc="first",  # ì¤‘ë³µ ì²˜ë¦¬
        )
        .sort_index()
        .reset_index()
    )

    print("ê°€ê³„ëŒ€ì¶œ pivot data")
    print(household_loan_df)

    return household_loan_df


# ======================================================================
# 6) ì€í—¹ëŒ€ì¶œ ë°ì´í„°
# ======================================================================

def load_bank_loan(data_dir: str) -> pd.DataFrame:
    # path = os.path.join(data_dir, "*_ëŒ€ì¶œê¸ˆ(ë§ì”).csv")
    path = find_file_by_pattern(data_dir, "ECOS_ì˜ˆê¸ˆì€í–‰_ì§€ì—­ë³„_ì€í–‰ëŒ€ì¶œ.csv")

    df = pd.read_csv(path, encoding="UTF-8-SIG", sep='\t')

    # ê³„ì •í•­ëª©ì´ 'ì›í™”ëŒ€ì¶œê¸ˆ' ì¸ í–‰ë§Œ
    df = df[df["ê³„ì •í•­ëª©ë³„"].str.contains("ì›í™”ëŒ€ì¶œê¸ˆ", na=False)]

    date_cols = df.columns[3:]

    melted = df.melt(
        id_vars=["ì§€ì—­ì½”ë“œë³„"],
        value_vars=date_cols,
        var_name="ë‚ ì§œ",
        value_name="ëŒ€ì¶œê¸ˆì•¡",
    )

    melted["ë‚ ì§œ"] = pd.to_datetime(
        melted["ë‚ ì§œ"].astype(str),
        format="%Y.%m",
        errors="coerce",
    ) + pd.offsets.MonthBegin(0)

    bank_loan_df = (
        melted.pivot_table(index="ë‚ ì§œ", columns="ì§€ì—­ì½”ë“œë³„", values="ëŒ€ì¶œê¸ˆì•¡", aggfunc="first")
        .sort_index()
        .reset_index()
    )

    for c in bank_loan_df.columns[1:]:
        bank_loan_df[c] = pd.to_numeric(bank_loan_df[c], errors="coerce")

    print("ì€í—¹ëŒ€ì¶œ data")
    print(bank_loan_df)

    return bank_loan_df


# ======================================================================
# 7) ê°œì¸ì†Œë“ ë°ì´í„°
# ======================================================================

def load_income(data_dir: str) -> pd.DataFrame:
    # path = os.path.join(data_dir, "*_ê°œì¸ì†Œë“.csv")
    path = find_file_by_pattern(data_dir, "*_ê°œì¸ì†Œë“.csv")

    income_csv = pd.read_csv(path, encoding='cp949', header=[0, 1])

    # ë©€í‹°ì»¬ëŸ¼ ì •ë¦¬
    income_csv.columns = pd.MultiIndex.from_tuples(
        [(str(a).strip(), str(b).strip()) for a, b in income_csv.columns]
    )

    regions = income_csv[("ì‹œë„ë³„", "ì‹œë„ë³„")].astype(str).str.strip()

    # '1ì¸ë‹¹ ê°œì¸ì†Œë“' ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    personal_cols = [
        c for c in income_csv.columns if c[1] == "1ì¸ë‹¹ ê°œì¸ì†Œë“"
    ]

    def extract_year(col):
        return int(re.sub(r"[^0-9]", "", col[0]))

    personal_cols = sorted(personal_cols, key=extract_year)

    monthly_list = []

    for col in personal_cols:
        raw_year = col[0]
        year = int(re.sub(r"[^0-9]", "", raw_year))

        annual_income = income_csv[col].astype(float)

        # í•´ë‹¹ ì—°ë„ì˜ ì›”ì´ˆ ë‚ ì§œ 12ê°œ
        dates = pd.date_range(f"{year}-01-01", f"{year}-12-01", freq="MS")
        monthly = pd.DataFrame({"ë‚ ì§œ": dates})

        # ì‹œë„ë³„ë¡œ ì—°ì†Œë“ / 12 / 10 (ë‹¨ìœ„: ë§Œì›)
        for region, value in zip(regions, annual_income):
            monthly[region] = np.round(float(value) / 12 / 10, 1)

        monthly_list.append(monthly)

    income_df = pd.concat(monthly_list, ignore_index=True)

    print("ê°œì¸ì†Œë“ data")
    print(income_df)

    return income_df


# ======================================================================
# 8) ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ + ìœ„ì¹˜ì½”ë“œ ë°ì´í„° ë¡œë“œ
# ======================================================================

def load_apartment_deal_and_location(data_dir: str):
    deal_csv_path = prepare_csv_from_zip(data_dir, "KoreaApartDeal.csv", "KoreaApartDeal.zip")
    loc_csv_path = prepare_csv_from_zip(data_dir, "LocationCode.csv", "LocationCode.zip")

    try:
        deal_df = pd.read_csv(deal_csv_path, low_memory=False)
        location_df = pd.read_csv(
            loc_csv_path,
            dtype={"ë²•ì •ë™ì½”ë“œ": str, "ìë©´ë™ëª…": str, "ë¦¬ëª…": str},
        )
    except Exception as e:
        print(f"file read error: {e}")
        exit()

    print(f"\n# of Total Apartment Deal Data: {len(deal_df):,}")

    # ê±°ë˜ì¼/ê±°ë˜ê¸ˆì•¡ ê²°ì¸¡ ì œê±°
    initial_rows = len(deal_df)
    deal_df.dropna(subset=["ê±°ë˜ì¼", "ê±°ë˜ê¸ˆì•¡"], inplace=True)
    final_rows = len(deal_df)

    # ì‹œêµ°êµ¬ëª… ë§¤í•‘
    location_df_filtered = location_df[location_df["ì‹œêµ°êµ¬ëª…"].notna()].copy()
    location_df_filtered["ì§€ì—­ì½”ë“œ"] = location_df_filtered["ë²•ì •ë™ì½”ë“œ"].str[:5].astype(int)
    loc_map = location_df_filtered[["ì§€ì—­ì½”ë“œ", "ì‹œë„ëª…", "ì‹œêµ°êµ¬ëª…"]].drop_duplicates()

    df = pd.merge(deal_df, loc_map, on="ì§€ì—­ì½”ë“œ", how="left")

    # ë²•ì •ë™ ì½”ë“œ ë§¤í•‘
    loc_lookup = location_df[["ì‹œë„ëª…", "ì‹œêµ°êµ¬ëª…", "ìë©´ë™ëª…", "ë¦¬ëª…", "ë²•ì •ë™ì½”ë“œ"]].copy()
    loc_lookup["ë²•ì •ë™"] = (
            loc_lookup["ìë©´ë™ëª…"].fillna("") + " " + loc_lookup["ë¦¬ëª…"].fillna("")
    ).str.strip()

    final_df = pd.merge(
        df,
        loc_lookup[["ì‹œë„ëª…", "ì‹œêµ°êµ¬ëª…", "ë²•ì •ë™", "ë²•ì •ë™ì½”ë“œ"]],
        on=["ì‹œë„ëª…", "ì‹œêµ°êµ¬ëª…", "ë²•ì •ë™"],
        how="left",
    )

    # UniqueID êµ¬ì„±
    final_df["ì•„íŒŒíŠ¸ID"] = pd.factorize(final_df["ì•„íŒŒíŠ¸"])[0]
    final_df["ì•„íŒŒíŠ¸ID"] = final_df["ì•„íŒŒíŠ¸ID"].astype(str).str.zfill(5)
    final_df["ì „ìš©ë©´ì ID"] = final_df["ì „ìš©ë©´ì "].round(0).astype(int).astype(str).str.zfill(3)
    final_df["UniqueID"] = final_df["ë²•ì •ë™ì½”ë“œ"] + final_df["ì•„íŒŒíŠ¸ID"] + final_df["ì „ìš©ë©´ì ID"]

    # ê±°ë˜ê¸ˆì•¡ ì •ë¦¬
    print("\n--- Cleaning and standardizing the 'ê±°ë˜ê¸ˆì•¡' column ---")
    final_df["ê±°ë˜ê¸ˆì•¡"] = pd.to_numeric(
        final_df["ê±°ë˜ê¸ˆì•¡"].astype(str).str.replace(",", ""),
        errors="coerce",
    )

    # ê±°ë˜ì¼ ì •ë¦¬
    print("\n--- Cleaning and standardizing the 'ê±°ë˜ì¼' column ---")
    final_df["ê±°ë˜ì¼_ì •ë¦¬"] = final_df["ê±°ë˜ì¼"].astype(str).str.split(" ").str[0]
    final_df["ê±°ë˜ì¼_ì •ë¦¬"] = pd.to_datetime(
        final_df["ê±°ë˜ì¼_ì •ë¦¬"],
        format="mixed",
        errors="coerce",
    )

    invalid_dates = final_df[final_df["ê±°ë˜ì¼_ì •ë¦¬"].isnull()]
    if not invalid_dates.empty:
        print("\n[Warning] The following rows could not be converted to a valid date:")
        print(invalid_dates[["ê±°ë˜ì¼"]])

    final_df.dropna(subset=["ê±°ë˜ì¼_ì •ë¦¬"], inplace=True)
    final_df["ê±°ë˜ì¼"] = final_df["ê±°ë˜ì¼_ì •ë¦¬"].dt.date
    final_df.drop(columns=["ê±°ë˜ì¼_ì •ë¦¬"], inplace=True)

    final_columns = [
        "UniqueID", "ì‹œë„ëª…", "ì‹œêµ°êµ¬ëª…", "ë²•ì •ë™", "ì•„íŒŒíŠ¸",
        "ì „ìš©ë©´ì ", "ê±°ë˜ì¼", "ê±°ë˜ê¸ˆì•¡", "ì¸µ", "ê±´ì¶•ë…„ë„",
    ]
    final_columns_exist = [col for col in final_columns if col in final_df.columns]
    final_df = final_df[final_columns_exist]

    # ğŸ”¥ ë‹¤ìš´ìºìŠ¤íŒ… ì ìš©
    final_df = reduce_memory_usage(final_df)

    print("\n'UniqueID' and apart deal data")
    print(final_df)

    return final_df


def build_apt_price_with_macro():
    # ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ
    rate_df = load_base_rate(DATA_DIR)
    population_df = load_population(DATA_DIR)
    unemployment_df = load_unemployment(DATA_DIR)
    cpi_df = load_cpi(DATA_DIR)
    household_loan_df = load_household_loan(DATA_DIR)
    bank_loan_df = load_bank_loan(DATA_DIR)
    income_df = load_income(DATA_DIR)

    print("\n--- Visualization Trend Charts ---")
    plot_macro_trends(population_df, unemployment_df, cpi_df, household_loan_df, bank_loan_df, rate_df, output_dir='preprocessed')


    # ğŸ”¥ ë‹¤ìš´ìºìŠ¤íŒ…
    rate_df = reduce_memory_usage(rate_df)
    population_df = reduce_memory_usage(population_df)
    unemployment_df = reduce_memory_usage(unemployment_df)
    cpi_df = reduce_memory_usage(cpi_df)
    household_loan_df = reduce_memory_usage(household_loan_df)
    bank_loan_df = reduce_memory_usage(bank_loan_df)
    income_df = reduce_memory_usage(income_df)

    # ì‹œë„ëª…/ì—´ í‘œì¤€í™”
    std_household_loan_df = standardize_columns(household_loan_df)
    std_population_df = standardize_columns(population_df)
    std_unemployment_df = standardize_columns(unemployment_df)
    std_bank_loan_df = standardize_columns(bank_loan_df)
    std_income_df = standardize_columns(income_df)

    # ë‚ ì§œ í˜•ì‹ í†µì¼
    for df_tmp in [std_household_loan_df, std_population_df, std_unemployment_df, rate_df, cpi_df, std_bank_loan_df, std_income_df]:
        df_tmp["ë‚ ì§œ"] = pd.to_datetime(df_tmp["ë‚ ì§œ"], errors="coerce")

    # ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜/ìœ„ì¹˜ ë°ì´í„°
    final_df = load_apartment_deal_and_location(DATA_DIR)

    # ì•„íŒŒíŠ¸ ê°€ê²© DF
    apt_price_df = final_df.copy()

    # ì‹œë„ëª… í‘œì¤€í™” (ê°’ ê¸°ì¤€)
    apt_price_df["ì‹œë„ëª…"] = apt_price_df["ì‹œë„ëª…"].replace(RENAME_MAP)
    apt_price_df = apt_price_df.dropna(subset=["ì‹œë„ëª…"])

    # ê±°ë˜ì¼ datetime ë³€í™˜
    apt_price_df["ê±°ë˜ì¼"] = pd.to_datetime(apt_price_df["ê±°ë˜ì¼"], errors="coerce")
    # print(apt_price_df["ê±°ë˜ì¼"].dtype)

    # ê±´ì¶•ë…„ë„ ê²°ì¸¡ ì±„ìš°ê¸°
    apt_price_df["ê±´ì¶•ë…„ë„"] = apt_price_df["ê±´ì¶•ë…„ë„"].fillna(2021)

    # ê¸°ì¤€ê¸ˆë¦¬ asof ë¨¸ì§€
    apt_price_df = pd.merge_asof(
        apt_price_df.sort_values("ê±°ë˜ì¼"),
        rate_df.sort_values("ë‚ ì§œ"),
        left_on="ê±°ë˜ì¼",
        right_on="ë‚ ì§œ",
        direction="forward",
    ).drop(columns=["ë‚ ì§œ"])

    # ê±°ë˜ì¼ì„ ì›”ì´ˆ ê¸°ì¤€ìœ¼ë¡œ (ì›” ë‹¨ìœ„ ë§¤í¬ë¡œì™€ align)
    apt_price_df["ì›”ê¸°ì¤€"] = apt_price_df["ê±°ë˜ì¼"].values.astype("datetime64[M]")
    # print(apt_price_df["ê±°ë˜ì¼"].dtype)

    # -------------------------
    # ê°€ê³„ëŒ€ì¶œê¸ˆ í•©ì¹˜ê¸°
    # -------------------------
    print("ê°€ê³„ëŒ€ì¶œê¸ˆ í•©ì¹˜ê¸° start")
    loan_long = std_household_loan_df.melt(
        id_vars=["ë‚ ì§œ"],
        var_name="ì‹œë„ëª…",
        value_name="ê°€ê³„ëŒ€ì¶œ(ë§Œì›)",
    )

    apt_price_df = pd.merge(
        apt_price_df,
        loan_long,
        left_on=["ì›”ê¸°ì¤€", "ì‹œë„ëª…"],
        right_on=["ë‚ ì§œ", "ì‹œë„ëª…"],
        how="left",
    ).drop(columns=["ë‚ ì§œ"])
    print("ê°€ê³„ëŒ€ì¶œê¸ˆ í•©ì¹˜ê¸° end")

    # -------------------------
    # ì€í–‰ëŒ€ì¶œê¸ˆ í•©ì¹˜ê¸°
    # -------------------------
    print("ì€í–‰ëŒ€ì¶œê¸ˆ í•©ì¹˜ê¸° start")
    loan_long = std_bank_loan_df.melt(
        id_vars=["ë‚ ì§œ"],
        var_name="ì‹œë„ëª…",
        value_name="ì€í–‰ëŒ€ì¶œ(ë§Œì›)",
    )

    apt_price_df = pd.merge(
        apt_price_df,
        loan_long,
        left_on=["ì›”ê¸°ì¤€", "ì‹œë„ëª…"],
        right_on=["ë‚ ì§œ", "ì‹œë„ëª…"],
        how="left",
    ).drop(columns=["ë‚ ì§œ"])

    print("ì€í–‰ëŒ€ì¶œê¸ˆ í•©ì¹˜ê¸° end")

    # -------------------------
    # ì¸êµ¬ìˆ˜ í•©ì¹˜ê¸°
    # -------------------------
    print("ì¸êµ¬ìˆ˜ í•©ì¹˜ê¸° start")
    pop_long = std_population_df.melt(
        id_vars=["ë‚ ì§œ"],
        var_name="ì‹œë„ëª…",
        value_name="ì¸êµ¬ìˆ˜",
    )

    apt_price_df = pd.merge(
        apt_price_df,
        pop_long,
        left_on=["ì›”ê¸°ì¤€", "ì‹œë„ëª…"],
        right_on=["ë‚ ì§œ", "ì‹œë„ëª…"],
        how="left",
    ).drop(columns=["ë‚ ì§œ"])
    print("ì¸êµ¬ìˆ˜ í•©ì¹˜ê¸° end")

    # -------------------------
    # ì‹¤ì—…ë¥  í•©ì¹˜ê¸°
    # -------------------------
    print("ì‹¤ì—…ë¥  í•©ì¹˜ê¸° start")
    unemp_long = std_unemployment_df.melt(
        id_vars=["ë‚ ì§œ"],
        var_name="ì‹œë„ëª…",
        value_name="ì‹¤ì—…ë¥ ",
    )

    apt_price_df = pd.merge(
        apt_price_df,
        unemp_long,
        left_on=["ì›”ê¸°ì¤€", "ì‹œë„ëª…"],
        right_on=["ë‚ ì§œ", "ì‹œë„ëª…"],
        how="left",
    ).drop(columns=["ë‚ ì§œ"])
    print("ì‹¤ì—…ë¥  í•©ì¹˜ê¸° end")

    # -------------------------
    # CPI (ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜) í•©ì¹˜ê¸°
    # -------------------------
    print("ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ í•©ì¹˜ê¸° start")
    cpi_df["ì›”ê¸°ì¤€"] = cpi_df["ë‚ ì§œ"].values.astype("datetime64[M]")

    apt_price_df = pd.merge(
        apt_price_df,
        cpi_df,
        on="ì›”ê¸°ì¤€",
        how="left",
    ).drop(columns=["ë‚ ì§œ"])
    print("ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ í•©ì¹˜ê¸° end")

    # -------------------------
    # ê°œì¸ì†Œë“ í•©ì¹˜ê¸°
    # -------------------------
    print("ê°œì¸ì†Œë“ í•©ì¹˜ê¸° start")
    income_long = std_income_df.melt(
        id_vars=["ë‚ ì§œ"],
        var_name="ì‹œë„ëª…",
        value_name="ì›”ê°œì¸ì†Œë“(ë§Œì›)",
    )

    apt_price_df = pd.merge(
        apt_price_df,
        income_long,
        left_on=["ì›”ê¸°ì¤€", "ì‹œë„ëª…"],
        right_on=["ë‚ ì§œ", "ì‹œë„ëª…"],
        how="left",
    ).drop(columns=["ë‚ ì§œ"])
    print("ê°œì¸ì†Œë“ í•©ì¹˜ê¸° end")

    apt_price_df = apt_price_df.drop('ì›”ê¸°ì¤€', axis=1)

    apt_price_df = reduce_memory_usage(apt_price_df)

    return apt_price_df

def set_font():
    os_name = platform.system()
    font_family = None

    if os_name == 'Windows':
        font_list = ['Malgun Gothic', 'Dotum', 'Gulim']
        for font in font_list:
            try:
                if fm.findfont(font, fallback_to_default=False):
                    font_family = font
                    break
            except:
                continue

        if font_family:
            plt.rc('font', family=font_family)
            print(f"'{font_family}' font is set for Windows.")
        else:
            print("Warning: Could not find a suitable Hangul font (Malgun Gothic, Dotum, Gulim) on this Windows system. Characters may appear broken.")

    elif os_name == 'Darwin':  # macOS
        font_list = ['Apple SD Gothic Neo', 'AppleGothic']
        for font in font_list:
            try:
                if fm.findfont(font, fallback_to_default=False):
                    font_family = font
                    break
            except:
                continue

        if font_family:
            plt.rc('font', family=font_family)
            print(f"'{font_family}' font is set for macOS.")
        else:
            print("Warning: Could not find a suitable Hangul font (Apple SD Gothic Neo, AppleGothic) on this macOS system. Characters may appear broken.")

    elif os_name == 'Linux':
        try:
            if fm.findfont('NanumGothic', fallback_to_default=False):
                plt.rc('font', family='NanumGothic')
                print("'NanumGothic' font is set for Linux.")
            else:
                print("Warning: 'NanumGothic' not found on this Linux system. Please install it. Characters may appear broken.")
        except Exception as e:
            print(f"Warning: An error occurred while setting 'NanumGothic'. Details: {e}")

    else:
        print("Warning: Could not determine the OS to set a proper Hangul font. Characters may appear broken.")

    plt.rcParams['axes.unicode_minus'] = False


def save_transaction_plots_by_date(df, output_dir='preprocessed'):
    os.makedirs(output_dir, exist_ok=True)
    print("\n--- Starting to extract daily transaction counts ---")
    daily_transaction_counts = df['ê±°ë˜ì¼'].value_counts()
    daily_transaction_counts_sorted = daily_transaction_counts.sort_index()

    plt.figure(figsize=(15, 7))
    daily_transaction_counts_sorted.plot(kind='line', color='royalblue')
    plt.title('ê±°ë˜ì¼ ë³„ ê±°ë˜ ê±´ìˆ˜ ì¶”ì´', fontsize=16)
    plt.xlabel('ê±°ë˜ì¼', fontsize=12)
    plt.ylabel('ê±°ë˜ ê±´ìˆ˜', fontsize=12)
    plt.margins(x=0.01)
    plt.ylim(bottom=0)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()

    output_viz_path = os.path.join(output_dir, 'ê±°ë˜ì¼ë³„_ê±°ë˜ê±´ìˆ˜_ì¶”ì´.png')
    plt.savefig(output_viz_path, dpi=150)
    print(f"Trend chart has been saved to: '{output_viz_path}'")

    plt.close()

def save_top100_plots_by_sido(df, output_dir='preprocessed'):
    os.makedirs(output_dir, exist_ok=True)
    print(f"transcation top 100 chart images are saved in '{output_dir}' folder.")

    sido_list = df['ì‹œë„ëª…'].unique()

    for sido in sido_list:
        print(f"\n'{sido}' chart is saving...")

        sido_df = df[df['ì‹œë„ëª…'] == sido].copy()

        if sido_df.empty:
            print(f"'{sido}' is empty. Skip!")
            continue

        transaction_counts = sido_df['UniqueID'].value_counts()
        top_100_ids = transaction_counts.head(100).index
        top_100_df = sido_df[sido_df['UniqueID'].isin(top_100_ids)]

        print(f"Graphing the top 100 apartment transactions out of a total of {len(transaction_counts):,} in the '{sido}' region.")

        plt.figure(figsize=(15, 8))
        ax = plt.gca()

        for unique_id in top_100_ids:
            target_df = top_100_df[top_100_df['UniqueID'] == unique_id].copy()
            if not target_df.empty:
                target_df.sort_values('ê±°ë˜ì¼', inplace=True)
                ax.plot(target_df['ê±°ë˜ì¼'], target_df['ê±°ë˜ê¸ˆì•¡'], marker='', linestyle='-', alpha=0.5)

        plt.title(f"'{sido}' ê±°ë˜ëŸ‰ ìƒìœ„ 100ê°œ ì•„íŒŒíŠ¸ ê°€ê²© ì¶”ì´", fontsize=16)
        plt.xlabel("ê±°ë˜ì¼", fontsize=12)
        plt.ylabel("ê±°ë˜ê¸ˆì•¡ (ë§Œì›)", fontsize=12)
        plt.ylim(bottom=0)
        ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, p: format(int(x), ',')))
        plt.margins(x=0.01)
        plt.xticks(rotation=45)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()

        safe_sido_name = "".join(c for c in sido if c.isalnum())
        filename = f"{safe_sido_name}_ê°€ê²©ì¶”ì´_ìƒìœ„100.png"
        filepath = os.path.join(output_dir, filename)

        plt.savefig(filepath, dpi=150)
        print(f"'{filepath}' file saved.")

        plt.close()

    print("\n--- All Chart is saved. ---")

def combine_images_to_grid(input_dir='preprocessed', output_filename='Combined_Apartment_Trends.png', except_filename='ì•„ì˜¤ì§€', grid_size=(4, 4)):
    print("\n--- Starting to combine plot images into a single grid image ---")

    try:
        image_files = [
            f for f in os.listdir(input_dir)
            if f.endswith('_ê°€ê²©ì¶”ì´_ìƒìœ„100.png') and not f.startswith(except_filename)
        ]
        image_files.sort()
        print("Excluding files starting with 'ì„œìš¸'.")
    except FileNotFoundError:
        print(f"Error: Input directory '{input_dir}' not found.")
        return

    if not image_files:
        print("No images found to combine after filtering.")
        return

    try:
        with Image.open(os.path.join(input_dir, image_files[0])) as img:
            img_width, img_height = img.size
    except Exception as e:
        print(f"Error opening the first image: {e}")
        return

    cols, rows = grid_size
    total_width = img_width * cols
    total_height = img_height * rows

    grid_image = Image.new('RGB', (total_width, total_height), 'white')
    print(f"Creating a new grid image with original size {total_width}x{total_height}...")

    for index, filename in enumerate(image_files):
        if index >= rows * cols:
            print(f"Warning: More than {rows * cols} images found. Only the first {rows * cols} will be included.")
            break

        row = index // cols
        col = index % cols
        x_offset = col * img_width
        y_offset = row * img_height

        try:
            with Image.open(os.path.join(input_dir, filename)) as img:
                grid_image.paste(img, (x_offset, y_offset))
        except Exception as e:
            print(f"Error processing {filename}: {e}")
            continue

    target_width, target_height = 3000, 1600
    print(f"Resizing final image to {target_width}x{target_height}...")
    resized_image = grid_image.resize((target_width, target_height), Image.Resampling.LANCZOS)

    output_path = os.path.join(input_dir, output_filename)
    resized_image.save(output_path, dpi=(150, 150))
    print(f"--- Successfully combined and resized {min(len(image_files), rows * cols)} images into '{output_path}' ---")


def plot_macro_trends(pop_df, unemp_df, cpi_df, household_df, bank_df, rate_df, output_dir='preprocessed'):

    # 1. í–‰ì •êµ¬ì—­ë³„ ì¸êµ¬ìˆ˜ ì¶”ì´
    plt.figure(figsize=(15, 8))
    for col in pop_df.columns[1:]:
        plt.plot(pop_df['ë‚ ì§œ'], pop_df[col], label=col, alpha=0.7)
    plt.title('í–‰ì •êµ¬ì—­ë³„ ì¸êµ¬ìˆ˜ ì¶”ì´', fontsize=16)
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('ì¸êµ¬ìˆ˜')
    plt.legend(ncol=3, fontsize=9)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'í–‰ì •êµ¬ì—­ë³„_ì¸êµ¬ìˆ˜_ì¶”ì´.png'), dpi=150)
    plt.close()

    # 2. í–‰ì •êµ¬ì—­ë³„ ì‹¤ì—…ë¥  ì¶”ì´
    plt.figure(figsize=(15, 8))
    for col in unemp_df.columns[1:]:
        plt.plot(unemp_df['ë‚ ì§œ'], unemp_df[col], label=col, alpha=0.7)
    plt.title('í–‰ì •êµ¬ì—­ë³„ ì‹¤ì—…ë¥  ì¶”ì´', fontsize=16)
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('ì‹¤ì—…ë¥ (%)')
    plt.legend(ncol=3, fontsize=9)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'í–‰ì •êµ¬ì—­ë³„_ì‹¤ì—…ë¥ _ì¶”ì´.png'), dpi=150)
    plt.close()

    # 3. ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI) ì¶”ì´
    plt.figure(figsize=(12, 6))
    plt.plot(cpi_df['ë‚ ì§œ'], cpi_df['CPI_ì´ì§€ìˆ˜'], color='royalblue', label='CPI ì´ì§€ìˆ˜')
    plt.title('ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI) ì¶”ì´', fontsize=16)
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('CPI ì´ì§€ìˆ˜')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜_ì¶”ì´.png'), dpi=150)
    plt.close()

    # 4. ì§€ì—­ë³„ ê°€ê³„ëŒ€ì¶œ ì¶”ì´
    plt.figure(figsize=(15, 8))
    for col in household_df.columns[1:]:
        plt.plot(household_df['ë‚ ì§œ'], household_df[col], label=col, alpha=0.7)
    plt.title('ì§€ì—­ë³„ ê°€ê³„ëŒ€ì¶œ ì¶”ì´', fontsize=16)
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('ê°€ê³„ëŒ€ì¶œ(ë§Œì›)')
    plt.legend(ncol=3, fontsize=9)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ì§€ì—­ë³„_ê°€ê³„ëŒ€ì¶œ_ì¶”ì´.png'), dpi=150)
    plt.close()

    # 5. ì§€ì—­ë³„ ì€í–‰ëŒ€ì¶œ ì¶”ì´
    plt.figure(figsize=(15, 8))
    for col in bank_df.columns[1:]:
        plt.plot(bank_df['ë‚ ì§œ'], bank_df[col], label=col, alpha=0.7)
    plt.title('ì§€ì—­ë³„ ì€í–‰ëŒ€ì¶œ ì¶”ì´', fontsize=16)
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('ì€í–‰ëŒ€ì¶œ(ë§Œì›)')
    plt.legend(ncol=3, fontsize=9)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ì§€ì—­ë³„_ì€í–‰ëŒ€ì¶œ_ì¶”ì´.png'), dpi=150)
    plt.close()

    # 6. í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ ì¶”ì´
    plt.figure(figsize=(12, 6))
    plt.plot(rate_df['ë‚ ì§œ'], rate_df['ê¸°ì¤€ê¸ˆë¦¬'], color='darkred', linewidth=2)
    plt.title('í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ ì¶”ì´', fontsize=16)
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('ê¸°ì¤€ê¸ˆë¦¬(%)')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'í•œêµ­ì€í–‰_ê¸°ì¤€ê¸ˆë¦¬_ì¶”ì´.png'), dpi=150)
    plt.close()

    print(f"ë¶€ê°€ë°ì´í„° ì‹œê³„ì—´ ì°¨íŠ¸ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":

    os.makedirs(PREPROCESSED_DIR, exist_ok=True)
    set_font()

    apt_price_df = build_apt_price_with_macro()
    print("\nall data merged apartment price data frame:")
    print(apt_price_df)

    output_file = 'KoreaApartDeal_PreProcessed.csv'
    print("\n writing", output_file)
    output_path = os.path.join(PREPROCESSED_DIR, output_file)
    apt_price_df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nwritten done: {output_path}")

    output_zip_file = output_file.replace('.csv', '.zip')
    output_zip_path = os.path.join(PREPROCESSED_DIR, output_zip_file)

    print(f"'Compressing {output_file}' to '{output_zip_file}' ...")
    create_split_zip(output_path, output_zip_path, 40)

    print("\n--- Starting Data Visualization ---")

    save_transaction_plots_by_date(apt_price_df, 'preprocessed')
    save_top100_plots_by_sido(apt_price_df, 'preprocessed')

    combine_images_to_grid(input_dir='preprocessed', output_filename='16ê°œì§€ì—­-top100.png', except_filename='ì„œìš¸', grid_size=(4, 4))

    print("\n--- Finish Data Visualization ---")
