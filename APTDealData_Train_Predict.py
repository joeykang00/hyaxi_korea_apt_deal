# pip install pandas numpy scikit-learn xgboost joblib matplotlib seaborn pillow

import pandas as pd
import os
import zipfile
import sys
import joblib
import platform
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from PIL import Image

TRAINED_DATA_DIR = 'trained_data'
MODEL_FILE_PATH = os.path.join(TRAINED_DATA_DIR, 'xgb_apartment_model.joblib')
PRELOAD_FILE_PATH = os.path.join(TRAINED_DATA_DIR, 'preload_xgb_data.csv')
ENCODERS_FILE_PATH = os.path.join(TRAINED_DATA_DIR, 'label_encoders.joblib')
PLOT_OUTPUT_DIR = 'results'  # ê·¸ë˜í”„ ë° ì¶”ì²œ ê²°ê³¼ ì €ì¥ í´ë”

SHOULD_RETRAIN = True
MIN_TRANSACTION_COUNT = 50  # ìµœì†Œ ê±°ë˜ íšŸìˆ˜ í•„í„°ë§ ê¸°ì¤€
cat_features = ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸']

# --------------

def set_font():
    os_name = platform.system()
    font_family = None

    if os_name == 'Windows':
        font_list = ['Malgun Gothic', 'Dotum', 'Gulim']
        for font in font_list:
            try:
                if fm.findfont(font, fallback_to_default=False):
                    font_family = font
                    break
            except:
                continue

        if font_family:
            plt.rc('font', family=font_family)
        else:
            print("Warning: Could not find a suitable Hangul font (Malgun Gothic, Dotum, Gulim) on this Windows system. Characters may appear broken.")

    elif os_name == 'Darwin':  # macOS
        font_list = ['Apple SD Gothic Neo', 'AppleGothic']
        for font in font_list:
            try:
                if fm.findfont(font, fallback_to_default=False):
                    font_family = font
                    break
            except:
                continue

        if font_family:
            plt.rc('font', family=font_family)
        else:
            print("Warning: Could not find a suitable Hangul font (Apple SD Gothic Neo, AppleGothic) on this macOS system. Characters may appear broken.")

    elif os_name == 'Linux':
        try:
            if fm.findfont('NanumGothic', fallback_to_default=False):
                plt.rc('font', family='NanumGothic')
            else:
                print("Warning: 'NanumGothic' not found on this Linux system. Please install it. Characters may appear broken.")
        except Exception as e:
            print(f"Warning: An error occurred while setting 'NanumGothic'. Details: {e}")

    else:
        print("Warning: Could not determine the OS to set a proper Hangul font. Characters may appear broken.")

    # ê³µí†µ: ìŒìˆ˜ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False


# **********************************************

def format_manwon(amount):
    # amountê°€ ë„˜íŒŒì´ float íƒ€ì…ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ intë¡œ ë³€í™˜
    if pd.isna(amount):
        return "N/A"
    return f"{int(amount):,.0f} ë§Œ ì›"


def prepare_csv_from_zip(data_dir, csv_filename, zip_filename):

    csv_path = os.path.join(data_dir, csv_filename)
    zip_path = os.path.join(data_dir, zip_filename)

    # ë¶„í•  ì••ì¶• íŒŒì¼ì˜ ì²« ë²ˆì§¸ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: KoreaApartDeal_PreProcessed.zip.001)
    split_zip_start = zip_path + ".001"

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    if not os.path.exists(csv_path):
        print(f"'{csv_path}' is not exist. Checking Zip file.")

        # 1. ë‹¨ì¼ Zip íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
        if os.path.exists(zip_path):
            print(f"'{zip_path}' is found. Unzip...")
            try:
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(data_dir)
                print(f"Completed Unzip. '{csv_path}' will be used.")
            except Exception as e:
                print(f"Unzip error: {e}")
                sys.exit(1)

        # 2. ë‹¨ì¼ Zipì€ ì—†ê³ , ë¶„í•  ì••ì¶• íŒŒì¼(.001)ì´ ìˆëŠ” ê²½ìš°
        elif os.path.exists(split_zip_start):
            print(f"Split zip file found ('{os.path.basename(split_zip_start)}'). Merging and Unzipping...")

            # ë³‘í•©ì„ ìœ„í•œ ì„ì‹œ íŒŒì¼
            temp_merged_zip = os.path.join(data_dir, "temp_merged_archive.zip")

            try:
                # (1) ë¶„í•  íŒŒì¼ ë³‘í•©
                with open(temp_merged_zip, 'wb') as merged_f:
                    part_num = 1
                    while True:
                        part_file = f"{zip_path}.{part_num:03d}"
                        if not os.path.exists(part_file):
                            break

                        # print(f"Merging {os.path.basename(part_file)}...")
                        with open(part_file, 'rb') as part_f:
                            merged_f.write(part_f.read())
                        part_num += 1

                # (2) ë³‘í•©ëœ íŒŒì¼ ì••ì¶• í•´ì œ
                print("Merging complete. Unzipping...")
                with zipfile.ZipFile(temp_merged_zip, 'r') as zip_ref:
                    zip_ref.extractall(data_dir)
                print(f"Completed Unzip. '{csv_path}' will be used.")

            except Exception as e:
                print(f"Merge/Unzip error: {e}")
                sys.exit(1)
            finally:
                # (3) ì„ì‹œ ë³‘í•© íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_merged_zip):
                    os.remove(temp_merged_zip)

        else:
            print(f"Error: '{csv_filename}' and '{zip_filename}' (or split parts) are not found.")
            print("Please ensure the data file is present.")
            sys.exit(1)

    return csv_path


def plot_apartment_timeseries(unique_id, original_df, reco_df, model, base_date, features, is_best=False):

    if 'font.family' not in plt.rcParams or not plt.rcParams['font.family']:
        set_font()

    if not os.path.exists(PLOT_OUTPUT_DIR):
        os.makedirs(PLOT_OUTPUT_DIR)

    apt_info = reco_df[reco_df['UniqueID'] == unique_id].iloc[0]
    past_df = original_df[original_df['UniqueID'] == unique_id].copy()

    if past_df.empty:
        print(f"ê²½ê³ : UniqueID {unique_id}ì— í•´ë‹¹í•˜ëŠ” ê³¼ê±° ê±°ë˜ ë°ì´í„°ê°€ ì›ë³¸ DFì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆí‚µë‹ˆë‹¤.")
        return

    # ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    base_row = past_df.sort_values(by='ê±°ë˜ì¼', ascending=True).iloc[-1].copy()
    start_date = pd.to_datetime(base_row['ê±°ë˜ì¼'])
    end_date = pd.to_datetime('2031-01-01')
    date_range = pd.date_range(start=start_date + pd.offsets.MonthBegin(1), end=end_date, freq='MS')

    future_data = []
    encoded_values = {
        'ì‹œë„ëª…': base_row['ì‹œë„ëª…'],
        'ì‹œêµ°êµ¬ëª…': base_row['ì‹œêµ°êµ¬ëª…'],
        'ë²•ì •ë™': base_row['ë²•ì •ë™'],
        'ì•„íŒŒíŠ¸': base_row['ì•„íŒŒíŠ¸'],
        'ì „ìš©ë©´ì ': base_row['ì „ìš©ë©´ì '],
        'ê±´ì¶•ë…„ë„': base_row['ê±´ì¶•ë…„ë„'],
	    'ê¸°ì¤€ê¸ˆë¦¬': base_row['ê¸°ì¤€ê¸ˆë¦¬'],
        'ê°€ê³„ëŒ€ì¶œ(ë§Œì›)': base_row['ê°€ê³„ëŒ€ì¶œ(ë§Œì›)'],
        'ì¸êµ¬ìˆ˜': base_row['ì¸êµ¬ìˆ˜'],
        'ì‹¤ì—…ë¥ ': base_row['ì‹¤ì—…ë¥ '],
        'CPI_ì´ì§€ìˆ˜': base_row['CPI_ì´ì§€ìˆ˜'],
        'CPI_ì „ë…„ë™ê¸°': base_row['CPI_ì „ë…„ë™ê¸°'],
        'ì›”ê°œì¸ì†Œë“(ë§Œì›)': base_row['ì›”ê°œì¸ì†Œë“(ë§Œì›)'],
    }

    for date in date_range:
        encoded_data = encoded_values.copy()
        encoded_data['ê±°ë˜_ë…„'] = date.year
        encoded_data['ê±°ë˜_ì›”'] = date.month
        encoded_data['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = date.year - base_row['ê±´ì¶•ë…„ë„']
        encoded_data['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (date - base_date).days
        future_data.append(encoded_data)

    if not future_data: return

    future_X = pd.DataFrame(future_data)[features]
    future_prices = model.predict(future_X)

    future_df = pd.DataFrame({
        'ê±°ë˜ì¼': date_range,
        'ê±°ë˜ê¸ˆì•¡': future_prices.astype(int)
    })

    plot_past_df = past_df[['ê±°ë˜ì¼', 'ê±°ë˜ê¸ˆì•¡']].copy()
    plot_past_df['ê±°ë˜ì¼'] = pd.to_datetime(plot_past_df['ê±°ë˜ì¼'])

    marker_buy = future_df[future_df['ê±°ë˜ì¼'] == '2026-01-01']
    marker_sell = future_df[future_df['ê±°ë˜ì¼'] == '2030-01-01']

    plt.figure(figsize=(14, 7))
    sns.lineplot(x='ê±°ë˜ì¼', y='ê±°ë˜ê¸ˆì•¡', data=future_df, label='ì˜ˆìƒ ê°€ê²© ì‹œê³„ì—´', color='orange', linestyle='--', linewidth=2)
    sns.scatterplot(x='ê±°ë˜ì¼', y='ê±°ë˜ê¸ˆì•¡', data=plot_past_df, label='ê³¼ê±° ì‹¤ì œ ê±°ë˜ ê°€ê²©', color='blue', s=50, zorder=5)

    if not marker_buy.empty:
        mbuy = marker_buy.iloc[0]
        plt.scatter(mbuy['ê±°ë˜ì¼'], mbuy['ê±°ë˜ê¸ˆì•¡'], color='red', s=100, zorder=10, label='2026ë…„ 1ì›” ë§¤ì… ì˜ˆìƒê°€')
        plt.annotate(
            f'ë§¤ì… ì˜ˆìƒê°€: {format_manwon(mbuy["ê±°ë˜ê¸ˆì•¡"])}',
            (mbuy['ê±°ë˜ì¼'], mbuy['ê±°ë˜ê¸ˆì•¡']),
            textcoords="offset points",
            xytext=(-30, 15), ha='center', color='red', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", fc="yellow", alpha=0.5)
        )
    if not marker_sell.empty:
        msell = marker_sell.iloc[0]
        plt.scatter(msell['ê±°ë˜ì¼'], msell['ê±°ë˜ê¸ˆì•¡'], color='green', s=100, zorder=10, label='2030ë…„ 1ì›” ë§¤ê° ì˜ˆìƒê°€')
        plt.annotate(
            f'ë§¤ê° ì˜ˆìƒê°€: {format_manwon(msell["ê±°ë˜ê¸ˆì•¡"])}',
            (msell['ê±°ë˜ì¼'], msell['ê±°ë˜ê¸ˆì•¡']),
            textcoords="offset points",
            xytext=(30, 15), ha='center', color='green', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", fc="lightgreen", alpha=0.5)
        )

    title = f"[{apt_info['ì‹œë„ëª…']} {apt_info['ì‹œêµ°êµ¬ëª…']} {apt_info['ë²•ì •ë™']}] {apt_info['ì•„íŒŒíŠ¸']} ({apt_info['ì „ìš©ë©´ì ']:.2f}mÂ²) ê°€ê²© ì‹œê³„ì—´ (ë‹¨ìœ„: ë§Œ ì›)"
    plt.title(title, fontsize=16)
    plt.xlabel("ê±°ë˜ì¼", fontsize=12)
    plt.ylabel("ê±°ë˜ ê¸ˆì•¡ (ë§Œ ì›)", fontsize=12)
    plt.gca().get_yaxis().set_major_formatter(ticker.FuncFormatter(lambda x, p: format_manwon(x)))
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xticks(rotation=45)
    plt.tight_layout()

    # íŒŒì¼ ì €ì¥ (ì´ë¦„ ê·œì¹™ ë³€ê²½)
    # is_bestê°€ Trueì¸ ê²½ìš°: TS_BEST_ì‹œë„ëª…_...png
    # is_bestê°€ Falseì¸ ê²½ìš°: TS_ì‹œë„ëª…_...png (ë³‘í•© ëŒ€ìƒ)
    prefix = 'TS_BEST' if is_best else 'TS'
    file_name = f"{prefix}_{apt_info['ì‹œë„ëª…']}_{apt_info['ì‹œêµ°êµ¬ëª…']}_{apt_info['ë²•ì •ë™']}_{apt_info['ì•„íŒŒíŠ¸']}_{apt_info['ì „ìš©ë©´ì ']:.2f}m2.png".replace('/', '_').replace('.', '_')
    save_path = os.path.join(PLOT_OUTPUT_DIR, file_name)
    plt.savefig(save_path)
    plt.close()
    return file_name


def combine_images_to_grid(input_dir, output_filename, except_filename, grid_size=(3, 3)):

    print("\n--- Starting to combine plot images into a single grid image ---")


    except_prefix = f"TS_BEST_{except_filename}"
    try:
        image_files = [
            f for f in os.listdir(input_dir)
            if f.startswith('TS_') and not f.startswith(except_prefix) and f.endswith('.png')
        ]
        image_files.sort()
        print(f"Excluding files starting with '{except_prefix}'. {len(image_files)} images found for combination.")
    except FileNotFoundError:
        print(f"Error: Input directory '{input_dir}' not found.")
        return

    if not image_files:
        print("No images found to combine after filtering.")
        return

    try:
        with Image.open(os.path.join(input_dir, image_files[0])) as img:
            img_width, img_height = img.size
    except Exception as e:
        print(f"Error opening the first image: {e}")
        return

    cols, rows = grid_size
    num_images_to_combine = min(len(image_files), rows * cols)

    total_width = img_width * cols
    total_height = img_height * rows

    grid_image = Image.new('RGB', (total_width, total_height), 'white')
    print(f"Creating a new {cols}x{rows} grid image with individual size {img_width}x{img_height}...")

    for index, filename in enumerate(image_files):
        if index >= num_images_to_combine:
            break

        row = index // cols
        col = index % cols
        x_offset = col * img_width
        y_offset = row * img_height

        try:
            with Image.open(os.path.join(input_dir, filename)) as img:
                grid_image.paste(img, (x_offset, y_offset))
        except Exception as e:
            print(f"Error processing {filename}: {e}")
            continue


    target_width, target_height = 3000, 1800  # 3x3 ê·¸ë¦¬ë“œì— ì í•©í•œ ê°€ë¡œë¡œ ê¸´ ë¹„ìœ¨
    print(f"Resizing final image to {target_width}x{target_height}...")
    resized_image = grid_image.resize((target_width, target_height), Image.LANCZOS)

    output_path = os.path.join(input_dir, output_filename)
    resized_image.save(output_path, dpi=(150, 150))
    print(f"--- Successfully combined and resized {num_images_to_combine} images into '{output_path}' ---")

    # # ë³‘í•©ì— ì‚¬ìš©ëœ ê°œë³„ íŒŒì¼ ì‚­ì œ
    # print("Deleting temporary individual plot files...")
    # for filename in image_files[:num_images_to_combine]:
    #     os.remove(os.path.join(input_dir, filename))
    # print("Temporary files deleted.")


def predict_region(df, selected_sido, xgb_model, features, cat_features, label_encoders, base_deal_date):

    encoded_sido = label_encoders['ì‹œë„ëª…'].transform([selected_sido])[0]

    region_df = df[df['ì‹œë„ëª…'] == encoded_sido].copy()

    all_unique_apts = region_df.drop_duplicates(subset=['UniqueID']).reset_index(drop=True)
    total = len(all_unique_apts)
    print(f"\n[{selected_sido}] ì§€ì—­ì˜ {total}ê°œ ê³ ìœ  ì•„íŒŒíŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ ë°ì´í„° ìƒì„± ì‹œì‘.")

    predict_date_buy = pd.to_datetime('2026-01-01')
    predict_date_sell = pd.to_datetime('2030-01-01')

    records = []

    for idx, apt in all_unique_apts.iterrows():
        apt_id = apt['UniqueID']
        past_df = region_df[region_df['UniqueID'] == apt_id].copy()
        if past_df.empty:
            continue

        # ì§„í–‰ìƒí™© í‘œì‹œ
        print(f"\r[ {idx + 1} / {total} ] ì²˜ë¦¬ ì¤‘...", end='')

        base_row = past_df.sort_values(by='ê±°ë˜ì¼', ascending=True).iloc[-1].copy()

        encoded_values = {
            'UniqueID': apt_id,
            'ì‹œë„ëª…': base_row['ì‹œë„ëª…'],
            'ì‹œêµ°êµ¬ëª…': base_row['ì‹œêµ°êµ¬ëª…'],
            'ë²•ì •ë™': base_row['ë²•ì •ë™'],
            'ì•„íŒŒíŠ¸': base_row['ì•„íŒŒíŠ¸'],
            'ì „ìš©ë©´ì ': base_row['ì „ìš©ë©´ì '],
            'ê±´ì¶•ë…„ë„': base_row['ê±´ì¶•ë…„ë„'],
            'ê¸°ì¤€ê¸ˆë¦¬': base_row['ê¸°ì¤€ê¸ˆë¦¬'],
            'ê°€ê³„ëŒ€ì¶œ(ë§Œì›)': base_row['ê°€ê³„ëŒ€ì¶œ(ë§Œì›)'],
            'ì¸êµ¬ìˆ˜': base_row['ì¸êµ¬ìˆ˜'],
            'ì‹¤ì—…ë¥ ': base_row['ì‹¤ì—…ë¥ '],
            'CPI_ì´ì§€ìˆ˜': base_row['CPI_ì´ì§€ìˆ˜'],
            'CPI_ì „ë…„ë™ê¸°': base_row['CPI_ì „ë…„ë™ê¸°'],
            'ì›”ê°œì¸ì†Œë“(ë§Œì›)': base_row['ì›”ê°œì¸ì†Œë“(ë§Œì›)'],
        }

        def make_feature_row(date):
            row = encoded_values.copy()
            row['ê±°ë˜_ë…„'] = date.year
            row['ê±°ë˜_ì›”'] = date.month
            row['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = date.year - base_row['ê±´ì¶•ë…„ë„']
            row['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (date - base_deal_date).days
            return row

        buy_X = pd.DataFrame([make_feature_row(predict_date_buy)])[features]
        sell_X = pd.DataFrame([make_feature_row(predict_date_sell)])[features]

        buy_price = int(xgb_model.predict(buy_X)[0])
        sell_price = int(xgb_model.predict(sell_X)[0])

        record = encoded_values.copy()
        record['ë§¤ì…ì˜ˆìƒê°€_2026_01'] = buy_price
        record['ë§¤ê°ì˜ˆìƒê°€_2030_01'] = sell_price
        record['ì˜ˆìƒ_ìµœëŒ€ì´ìµ'] = sell_price - buy_price
        records.append(record)

    print("\nëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ. ê²°ê³¼ DataFrame êµ¬ì„± ì¤‘...")

    reco_df = pd.DataFrame(records)

    for col in cat_features:
        if col in reco_df.columns:
            reco_df[col] = label_encoders[col].inverse_transform(reco_df[col].astype(int))

    reco_df = reco_df.sort_values(by='ì˜ˆìƒ_ìµœëŒ€ì´ìµ', ascending=False).reset_index(drop=True)
    print(f"[{selected_sido}] ì§€ì—­ ì˜ˆì¸¡ ì™„ë£Œ. ì´ {len(reco_df)}ê°œ ê²°ê³¼ ìƒì„±.")
    return reco_df


if __name__ == "__main__":

    set_font()

    if not os.path.exists(TRAINED_DATA_DIR):
        os.makedirs(TRAINED_DATA_DIR)
    if not os.path.exists(PLOT_OUTPUT_DIR):
        os.makedirs(PLOT_OUTPUT_DIR)

    data_dir = './preprocessed'
    data_csv_path = prepare_csv_from_zip(data_dir, 'KoreaApartDeal_PreProcessed.csv', 'KoreaApartDeal_PreProcessed.zip')

    if (os.path.exists(PRELOAD_FILE_PATH) and
            os.path.exists(ENCODERS_FILE_PATH) and
            not SHOULD_RETRAIN):
        print(f"\npreload file found: load '{PRELOAD_FILE_PATH}'.")
        try:
            dtype_spec_loaded = {col: 'int' for col in ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸']}
            df = pd.read_csv(PRELOAD_FILE_PATH, dtype=dtype_spec_loaded)
            df['ê±°ë˜ì¼'] = pd.to_datetime(df['ê±°ë˜ì¼'])
            label_encoders = joblib.load(ENCODERS_FILE_PATH)
            base_deal_date = df['ê±°ë˜ì¼'].min()
            print("Load ë°ì´í„° ë° LabelEncoder ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"load error: {e}. new loading start...")
            SHOULD_RETRAIN = True
    else:
        SHOULD_RETRAIN = True

    if SHOULD_RETRAIN:
        print("\nData load start for XGBoost...")
        try:
            dtype_spec = {'ì¸µ': 'object'}
            df = pd.read_csv(data_csv_path, dtype=dtype_spec)
        except Exception as e:
            print(f"file read error: {e}")
            sys.exit(1)

        df['ê±°ë˜ê¸ˆì•¡'] = pd.to_numeric(df['ê±°ë˜ê¸ˆì•¡'], errors='coerce')
        df.dropna(subset=['ê±°ë˜ê¸ˆì•¡'], inplace=True)

        core_features = ['ê±°ë˜ì¼', 'ê±´ì¶•ë…„ë„', 'ì „ìš©ë©´ì ', 'ê±°ë˜ê¸ˆì•¡', 'ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ê¸°ì¤€ê¸ˆë¦¬', 'ê°€ê³„ëŒ€ì¶œ(ë§Œì›)', 'ì¸êµ¬ìˆ˜', 'ì‹¤ì—…ë¥ ', 'CPI_ì´ì§€ìˆ˜', 'CPI_ì „ë…„ë™ê¸°', 'ì›”ê°œì¸ì†Œë“(ë§Œì›)']
        df.dropna(subset=core_features, inplace=True)

        # df['ì¸µ'] = pd.to_numeric(df['ì¸µ'], errors='coerce')
        # df.dropna(subset=['ì¸µ'], inplace=True)
        # df['ì¸µ'] = df['ì¸µ'].astype(int)

        df['ê±°ë˜ì¼'] = pd.to_datetime(df['ê±°ë˜ì¼'], errors='coerce')
        df.dropna(subset=['ê±°ë˜ì¼'], inplace=True)
        df['ê±´ì¶•ë…„ë„'] = df['ê±´ì¶•ë…„ë„'].astype(int)

        df['ê±°ë˜_ë…„'] = df['ê±°ë˜ì¼'].dt.year
        df['ê±°ë˜_ì›”'] = df['ê±°ë˜ì¼'].dt.month
        df['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = df['ê±°ë˜_ë…„'] - df['ê±´ì¶•ë…„ë„']
        base_deal_date = df['ê±°ë˜ì¼'].min()
        df['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (df['ê±°ë˜ì¼'] - base_deal_date).dt.days

        # ìµœì†Œ ê±°ë˜ íšŸìˆ˜ í•„í„°ë§
        apt_counts = df['UniqueID'].value_counts()
        valid_uids = apt_counts[apt_counts >= MIN_TRANSACTION_COUNT].index
        df = df[df['UniqueID'].isin(valid_uids)].copy()
        print(f"ìµœì†Œ {MIN_TRANSACTION_COUNT}íšŒ ì´ìƒ ê±°ë˜ëœ ì•„íŒŒíŠ¸ë¡œ í•„í„°ë§ í›„, ë‚¨ì€ ê±°ë˜ ê¸°ë¡: {len(df)}ê°œ")

        # Label Encoding ìˆ˜í–‰
        label_encoders = {}
        for col in cat_features:
            df[col] = df[col].astype(str).fillna('missing')
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            label_encoders[col] = le

        try:
            df.to_csv(PRELOAD_FILE_PATH, index=False, encoding='utf-8')
            joblib.dump(label_encoders, ENCODERS_FILE_PATH)
            print(f"'{PRELOAD_FILE_PATH}' saved.")
        except Exception as e:
            print(f"Load file error: {e}")

    features = ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ì „ìš©ë©´ì ', 'ê±´ì¶•ë…„ë„', 'ê±°ë˜_ë…„', 'ê±°ë˜_ì›”', 'ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜', 'ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜', 'ê¸°ì¤€ê¸ˆë¦¬', 'ê°€ê³„ëŒ€ì¶œ(ë§Œì›)', 'ì¸êµ¬ìˆ˜', 'ì‹¤ì—…ë¥ ', 'CPI_ì´ì§€ìˆ˜', 'CPI_ì „ë…„ë™ê¸°', 'ì›”ê°œì¸ì†Œë“(ë§Œì›)']    
    target = 'ê±°ë˜ê¸ˆì•¡'

    # ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ Xì— í• ë‹¹
    X = df[features]
    y = df[target]

    if os.path.exists(MODEL_FILE_PATH) and not SHOULD_RETRAIN:
        try:
            xgb_model = joblib.load(MODEL_FILE_PATH)
            print("XGBoost model loaded.")
        except Exception as e:
            print(f"error: {e}. try to train.")
            SHOULD_RETRAIN = True
    else:
        SHOULD_RETRAIN = True

    if SHOULD_RETRAIN:
        print("\nModel Training Started ..")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        xgb_model = XGBRegressor(
            n_estimators=1000,
            learning_rate=0.3,
            max_depth=6,
            random_state=42,
            n_jobs=-1
        )
        xgb_model.fit(X_train, y_train)
        print("XGBoost Model Training Done.")
        try:
            joblib.dump(xgb_model, MODEL_FILE_PATH)
            print(f"Trained Model '{MODEL_FILE_PATH}' is saved")
        except Exception as e:
            print(f"error: {e}")

    decoded_sido = pd.Series(label_encoders['ì‹œë„ëª…'].inverse_transform(df['ì‹œë„ëª…'].astype(int)))
    sido_list = sorted(decoded_sido.unique())
    sido_map = {i + 1: sido for i, sido in enumerate(sido_list)}

    while True:
        print("\n" + "=" * 50)
        print("ì§€ì—­ ì„ íƒ: 2026ë…„ 1ì›” ë§¤ì… ê¸°ì¤€ 2030ë…„ 1ì›” ë§¤ê°ì‹œ ê°€ì¥ ìµœëŒ€ì´ìµì„ ì˜ˆì¸¡í•  ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        print("0: í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print("=" * 50)
        for num, sido in sido_map.items():
            print(f"{num}: {sido}")
        print("=" * 50)

        selected_num = None
        try:
            user_input = input("ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0 ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
            if user_input.strip() == '':
                continue
            selected_num = int(user_input)
        except ValueError:
            print("ìœ íš¨í•œ ìˆ«ì í˜•ì‹ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue

        if selected_num == 0:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.")
            sys.exit(0)

        if selected_num not in sido_map:
            print("ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ëª©ë¡ì— ìˆëŠ” ë²ˆí˜¸(1 ì´ìƒ)ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue

        selected_sido = sido_map[selected_num]

        reco_df = predict_region(df, selected_sido, xgb_model, features, cat_features, label_encoders, base_deal_date)

        if reco_df.empty:
            print(f"\nê²½ê³ : {selected_sido} ì§€ì—­ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        filtered_df = reco_df[reco_df['ì‹œë„ëª…'] == selected_sido].copy()

        if filtered_df.empty:
            print(f"\nê²½ê³ : {selected_sido} ì§€ì—­ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            continue

        top_10_apts = filtered_df.head(10).copy()
        best_apt = top_10_apts.iloc[0]


        output_text = "\n" + "=" * 70 + "\n"
        output_text += f"{selected_sido} ìµœëŒ€ ì´ìµ ì•„íŒŒíŠ¸ ì¶”ì²œ ê²°ê³¼ (ë‹¨ìœ„: ë§Œ ì›)\n"
        output_text += "=" * 70 + "\n"
        output_text += f"**ìµœì  ì•„íŒŒíŠ¸:** {best_apt['ì•„íŒŒíŠ¸']} ({best_apt['ì‹œêµ°êµ¬ëª…']} {best_apt['ë²•ì •ë™']})\n"
        output_text += f"**ì „ìš©ë©´ì :** {best_apt['ì „ìš©ë©´ì ']:.2f} mÂ²\n"
        output_text += f"**2025ë…„ 12ì›” ì˜ˆìƒ ë§¤ì…ê°€:** {format_manwon(best_apt['ë§¤ì…ì˜ˆìƒê°€_2026_01'])}\n"
        output_text += f"**2030ë…„ 12ì›” ì˜ˆìƒ ë§¤ê°ê°€:** {format_manwon(best_apt['ë§¤ê°ì˜ˆìƒê°€_2030_01'])}\n"
        output_text += f"**ì˜ˆìƒ ìµœëŒ€ ì´ìµ (5ë…„):** {format_manwon(best_apt['ì˜ˆìƒ_ìµœëŒ€ì´ìµ'])}\n"
        output_text += "=" * 70 + "\n"

        output_text += f"\n### ğŸ™ï¸ ìƒìœ„ 10ê°œ ì¶”ì²œ ì•„íŒŒíŠ¸ ëª©ë¡ ({selected_sido}, ì´ìµ ë§Œ ì› ê¸°ì¤€)\n\n"

        display_cols = ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ì „ìš©ë©´ì ', 'ì˜ˆìƒ_ìµœëŒ€ì´ìµ', 'ë§¤ì…ì˜ˆìƒê°€_2026_01', 'ë§¤ê°ì˜ˆìƒê°€_2030_01']

        # ìˆ«ì í¬ë§· ì ìš©
        top_10_formatted = top_10_apts[display_cols].copy()
        top_10_formatted['ì˜ˆìƒ_ìµœëŒ€ì´ìµ'] = top_10_formatted['ì˜ˆìƒ_ìµœëŒ€ì´ìµ'].map('{:,.0f}'.format)
        top_10_formatted['ë§¤ì…ì˜ˆìƒê°€_2026_01'] = top_10_formatted['ë§¤ì…ì˜ˆìƒê°€_2026_01'].map('{:,.0f}'.format)
        top_10_formatted['ë§¤ê°ì˜ˆìƒê°€_2030_01'] = top_10_formatted['ë§¤ê°ì˜ˆìƒê°€_2030_01'].map('{:,.0f}'.format)
        top_10_formatted['ì „ìš©ë©´ì '] = top_10_formatted['ì „ìš©ë©´ì '].map('{:.2f}'.format)

        # Markdown í‘œ ìˆ˜ë™ ìƒì„±
        header = "| " + " | ".join(display_cols) + " |"
        separator = "| " + " | ".join([":---:" for _ in display_cols]) + " |"
        rows = [
            "| " + " | ".join(map(str, row)) + " |"
            for row in top_10_formatted.values
        ]
        top_10_md = "\n".join([header, separator] + rows)

        output_text += top_10_md + "\n"

        output_text += "\n" + "=" * 70 + "\n"
        output_text += f"ê²°ê³¼ëŠ” ./{PLOT_OUTPUT_DIR}/{selected_sido}_APT_Recommendation.txt íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        output_text += "ìµœì  ì•„íŒŒíŠ¸ëŠ” ê°œë³„ PNG íŒŒì¼ë¡œ, ë‚˜ë¨¸ì§€ 9ê°œëŠ” í•˜ë‚˜ì˜ PNG íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤."
        print(output_text)

        recommendation_file_name = f"{selected_sido}_APT_Recommendation.txt"
        recommendation_file_path = os.path.join(PLOT_OUTPUT_DIR, recommendation_file_name)
        try:
            with open(recommendation_file_path, 'w', encoding='utf-8') as f:
                f.write(output_text)
            print(f"ì¶”ì²œ ê²°ê³¼ê°€ '{recommendation_file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"error: {e}")

        # 1. ìµœëŒ€ ì´ìµ ì•„íŒŒíŠ¸ (top 1) ê°œë³„ ì €ì¥
        print(f"\nìµœì  ì•„íŒŒíŠ¸ ({best_apt['ì•„íŒŒíŠ¸']}) ì‹œê³„ì—´ ì°¨íŠ¸ ê°œë³„ ì €ì¥...")
        plot_apartment_timeseries(
            unique_id=best_apt['UniqueID'],
            original_df=df,
            reco_df=reco_df,
            model=xgb_model,
            base_date=base_deal_date,
            features=features,
            is_best=True  # ê°œë³„ íŒŒì¼ ì €ì¥ í”Œë˜ê·¸
        )
        best_apt_filename_prefix = f"{selected_sido}_{best_apt['ì‹œêµ°êµ¬ëª…']}_{best_apt['ë²•ì •ë™']}_{best_apt['ì•„íŒŒíŠ¸']}"

        print(f"\ní•˜ìœ„ 9ê°œ ì•„íŒŒíŠ¸ ì‹œê³„ì—´ ì°¨íŠ¸ ì„ì‹œ íŒŒì¼ ì €ì¥ ë° ë³‘í•© ì‹œì‘...")

        temp_files = []

        for i in range(1, len(top_10_apts)):  # ì¸ë±ìŠ¤ 1ë¶€í„° ì‹œì‘ (2ë²ˆì§¸ ì•„íŒŒíŠ¸ë¶€í„°)
            apt = top_10_apts.iloc[i]
            filename = plot_apartment_timeseries(
                unique_id=apt['UniqueID'],
                original_df=df,
                reco_df=reco_df,
                model=xgb_model,
                base_date=base_deal_date,
                features=features,
                is_best=False  # ë³‘í•©ìš© íŒŒì¼ ì €ì¥ í”Œë˜ê·¸
            )
            if filename:
                temp_files.append(filename)

        if temp_files:
            combine_images_to_grid(
                input_dir=PLOT_OUTPUT_DIR,
                output_filename=f"Combined_Top9_Trends_{selected_sido}.png",
                except_filename=best_apt_filename_prefix,  # TS_BEST_ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ ì œì™¸
                grid_size=(3, 3)  # 3x3 ê·¸ë¦¬ë“œ
            )
        else:
            print("warn..")

        print("#" * 70)

        input("ì§„í–‰í•˜ë ¤ë©´ ì•„ë¬´í‚¤ë‚˜ ì…ë ¥í•˜ì„¸ìš”")
