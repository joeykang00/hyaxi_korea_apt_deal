# pip install pandas numpy scikit-learn xgboost joblib matplotlib seaborn pillow

import pandas as pd
import os
import zipfile
import sys
import joblib
import platform
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from PIL import Image

# --- ì„¤ì • ë³€ìˆ˜ ---
# ëª¨ë¸ ë° ì „ì²˜ë¦¬ íŒŒì¼ ì €ì¥ í´ë”
TRAINED_DATA_DIR = 'trained_data'
MODEL_FILE_PATH = os.path.join(TRAINED_DATA_DIR, 'xgb_apartment_model.joblib')
PRELOAD_FILE_PATH = os.path.join(TRAINED_DATA_DIR, 'preload_xgb_data.csv')
ENCODERS_FILE_PATH = os.path.join(TRAINED_DATA_DIR, 'label_encoders.joblib')
PLOT_OUTPUT_DIR = 'results'  # ê·¸ë˜í”„ ë° ì¶”ì²œ ê²°ê³¼ ì €ì¥ í´ë”

# ì œì–´ í”Œë˜ê·¸
SHOULD_RETRAIN = True  # ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í• ì§€ ì—¬ë¶€ (ìµœì´ˆ ì‹¤í–‰ ì‹œ True ê¶Œì¥)
MIN_TRANSACTION_COUNT = 10  # ìµœì†Œ ê±°ë˜ íšŸìˆ˜ í•„í„°ë§ ê¸°ì¤€


# --------------

# **********************************************
# ** Matplotlib OSë³„ í•œê¸€ í°íŠ¸ ì„¤ì • í•¨ìˆ˜ **
# **********************************************
def set_font():
    """OSì— ë”°ë¼ ì ì ˆí•œ í•œê¸€ í°íŠ¸ë¥¼ ì„¤ì •í•˜ê³ , ìŒìˆ˜ ë¶€í˜¸ ê¹¨ì§ì„ ë°©ì§€í•©ë‹ˆë‹¤."""
    os_name = platform.system()
    font_family = None

    if os_name == 'Windows':
        font_list = ['Malgun Gothic', 'Dotum', 'Gulim']
        for font in font_list:
            try:
                if fm.findfont(font, fallback_to_default=False):
                    font_family = font
                    break
            except:
                continue

        if font_family:
            plt.rc('font', family=font_family)
        else:
            print("Warning: Could not find a suitable Hangul font (Malgun Gothic, Dotum, Gulim) on this Windows system. Characters may appear broken.")

    elif os_name == 'Darwin':  # macOS
        font_list = ['Apple SD Gothic Neo', 'AppleGothic']
        for font in font_list:
            try:
                if fm.findfont(font, fallback_to_default=False):
                    font_family = font
                    break
            except:
                continue

        if font_family:
            plt.rc('font', family=font_family)
        else:
            print("Warning: Could not find a suitable Hangul font (Apple SD Gothic Neo, AppleGothic) on this macOS system. Characters may appear broken.")

    elif os_name == 'Linux':
        try:
            if fm.findfont('NanumGothic', fallback_to_default=False):
                plt.rc('font', family='NanumGothic')
            else:
                print("Warning: 'NanumGothic' not found on this Linux system. Please install it. Characters may appear broken.")
        except Exception as e:
            print(f"Warning: An error occurred while setting 'NanumGothic'. Details: {e}")

    else:
        print("Warning: Could not determine the OS to set a proper Hangul font. Characters may appear broken.")

    # ê³µí†µ: ìŒìˆ˜ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False


# **********************************************

# --- ê¸ˆì•¡ í¬ë§·íŒ… í•¨ìˆ˜ ì •ì˜ (ë‹¨ìœ„: ë§Œ ì›) ---
def format_manwon(amount):
    """ê¸ˆì•¡ì„ ì„¸ ìë¦¬ ì‰¼í‘œì™€ 'ë§Œ ì›'ì„ í¬í•¨í•˜ì—¬ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    # amountê°€ ë„˜íŒŒì´ float íƒ€ì…ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ intë¡œ ë³€í™˜
    if pd.isna(amount):
        return "N/A"
    return f"{int(amount):,.0f} ë§Œ ì›"


# --- 1. ë°ì´í„° ë¡œë”© ë° ì¤€ë¹„ í•¨ìˆ˜ ---
def prepare_csv_from_zip(data_dir, csv_filename, zip_filename):
    """
    ì§€ì •ëœ ê²½ë¡œì—ì„œ CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ZIP íŒŒì¼ì—ì„œ ì••ì¶•ì„ í•´ì œí•©ë‹ˆë‹¤.
    """
    csv_path = os.path.join(data_dir, csv_filename)
    zip_path = os.path.join(data_dir, zip_filename)

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    if not os.path.exists(csv_path):
        print(f"'{csv_path}' is not exist. Checking Zip file.")
        if os.path.exists(zip_path):
            print(f"'{zip_path}' is found. Unzip...")
            try:
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(data_dir)
                print(f"Completed Unzip. '{csv_path}' will be used.")
            except Exception as e:
                print(f"Unzip error: {e}")
                sys.exit(1)
        else:
            print(f"Error: '{csv_filename}' and '{zip_filename}' are not found. Please ensure the data file is present.")
            sys.exit(1)
    return csv_path


# --- 2. ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ ë° ì‹œê°í™” í•¨ìˆ˜ ---
def plot_apartment_timeseries(unique_id, original_df, reco_df, model, base_date, features, is_best=False):
    """
    íŠ¹ì • UniqueIDë¥¼ ê°€ì§„ ì•„íŒŒíŠ¸ì˜ ê³¼ê±° ê±°ë˜ ë°ì´í„°ì™€ ë¯¸ë˜ ì˜ˆì¸¡ ê°€ê²©ì„ ì‹œê³„ì—´ë¡œ ì‹œê°í™”í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (ë‹¨ìœ„: ë§Œ ì›)
    is_best=Trueë©´ ê°œë³„ íŒŒì¼, Falseë©´ ë³‘í•©ìš© íŒŒì¼ ì´ë¦„ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.
    """
    if 'font.family' not in plt.rcParams or not plt.rcParams['font.family']:
        set_font()

    if not os.path.exists(PLOT_OUTPUT_DIR):
        os.makedirs(PLOT_OUTPUT_DIR)

    apt_info = reco_df[reco_df['UniqueID'] == unique_id].iloc[0]
    past_df = original_df[original_df['UniqueID'] == unique_id].copy()

    if past_df.empty:
        print(f"ê²½ê³ : UniqueID {unique_id}ì— í•´ë‹¹í•˜ëŠ” ê³¼ê±° ê±°ë˜ ë°ì´í„°ê°€ ì›ë³¸ DFì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆí‚µë‹ˆë‹¤.")
        return

    # ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    base_row = past_df.sort_values(by='ê±°ë˜ì¼', ascending=True).iloc[-1].copy()
    start_date = pd.to_datetime(base_row['ê±°ë˜ì¼'])
    end_date = pd.to_datetime('2031-01-01')
    date_range = pd.date_range(start=start_date + pd.offsets.MonthBegin(1), end=end_date, freq='MS')

    future_data = []
    encoded_values = {
        'ì‹œë„ëª…': base_row['ì‹œë„ëª…'],
        'ì‹œêµ°êµ¬ëª…': base_row['ì‹œêµ°êµ¬ëª…'],
        'ë²•ì •ë™': base_row['ë²•ì •ë™'],
        'ì•„íŒŒíŠ¸': base_row['ì•„íŒŒíŠ¸'],
        'ì „ìš©ë©´ì ': base_row['ì „ìš©ë©´ì '],
        'ê±´ì¶•ë…„ë„': base_row['ê±´ì¶•ë…„ë„'],
    }

    for date in date_range:
        encoded_data = encoded_values.copy()
        encoded_data['ê±°ë˜_ë…„'] = date.year
        encoded_data['ê±°ë˜_ì›”'] = date.month
        encoded_data['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = date.year - base_row['ê±´ì¶•ë…„ë„']
        encoded_data['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (date - base_date).days
        future_data.append(encoded_data)

    if not future_data: return

    future_X = pd.DataFrame(future_data)[features]
    future_prices = model.predict(future_X)

    future_df = pd.DataFrame({
        'ê±°ë˜ì¼': date_range,
        'ê±°ë˜ê¸ˆì•¡': future_prices.astype(int)
    })

    plot_past_df = past_df[['ê±°ë˜ì¼', 'ê±°ë˜ê¸ˆì•¡']].copy()
    plot_past_df['ê±°ë˜ì¼'] = pd.to_datetime(plot_past_df['ê±°ë˜ì¼'])

    marker_2025 = future_df[future_df['ê±°ë˜ì¼'] == '2025-12-01']
    marker_2030 = future_df[future_df['ê±°ë˜ì¼'] == '2030-12-01']

    # í”Œë¡¯ ì„¤ì •
    plt.figure(figsize=(14, 7))
    sns.lineplot(x='ê±°ë˜ì¼', y='ê±°ë˜ê¸ˆì•¡', data=future_df, label='ì˜ˆìƒ ê°€ê²© ì‹œê³„ì—´', color='orange', linestyle='--', linewidth=2)
    sns.scatterplot(x='ê±°ë˜ì¼', y='ê±°ë˜ê¸ˆì•¡', data=plot_past_df, label='ê³¼ê±° ì‹¤ì œ ê±°ë˜ ê°€ê²©', color='blue', s=50, zorder=5)

    # ì£¼ìš” ì˜ˆì¸¡ ì§€ì  í‘œì‹œ
    if not marker_2025.empty:
        m25 = marker_2025.iloc[0]
        plt.scatter(m25['ê±°ë˜ì¼'], m25['ê±°ë˜ê¸ˆì•¡'], color='red', s=100, zorder=10, label='2025ë…„ 12ì›” ë§¤ì… ì˜ˆìƒê°€')
        plt.annotate(
            f'ë§¤ì… ì˜ˆìƒê°€: {format_manwon(m25["ê±°ë˜ê¸ˆì•¡"])}',
            (m25['ê±°ë˜ì¼'], m25['ê±°ë˜ê¸ˆì•¡']),
            textcoords="offset points",
            xytext=(-30, 15), ha='center', color='red', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", fc="yellow", alpha=0.5)
        )
    if not marker_2030.empty:
        m30 = marker_2030.iloc[0]
        plt.scatter(m30['ê±°ë˜ì¼'], m30['ê±°ë˜ê¸ˆì•¡'], color='green', s=100, zorder=10, label='2030ë…„ 12ì›” ë§¤ê° ì˜ˆìƒê°€')
        plt.annotate(
            f'ë§¤ê° ì˜ˆìƒê°€: {format_manwon(m30["ê±°ë˜ê¸ˆì•¡"])}',
            (m30['ê±°ë˜ì¼'], m30['ê±°ë˜ê¸ˆì•¡']),
            textcoords="offset points",
            xytext=(30, 15), ha='center', color='green', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", fc="lightgreen", alpha=0.5)
        )

    # ì‹œê°í™” ì„¤ì •
    title = f"[{apt_info['ì‹œë„ëª…']} {apt_info['ì‹œêµ°êµ¬ëª…']} {apt_info['ë²•ì •ë™']}] {apt_info['ì•„íŒŒíŠ¸']} ({apt_info['ì „ìš©ë©´ì ']:.2f}mÂ²) ê°€ê²© ì‹œê³„ì—´ (ë‹¨ìœ„: ë§Œ ì›)"
    plt.title(title, fontsize=16)
    plt.xlabel("ê±°ë˜ì¼", fontsize=12)
    plt.ylabel("ê±°ë˜ ê¸ˆì•¡ (ë§Œ ì›)", fontsize=12)
    plt.gca().get_yaxis().set_major_formatter(ticker.FuncFormatter(lambda x, p: format_manwon(x)))
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xticks(rotation=45)
    plt.tight_layout()

    # íŒŒì¼ ì €ì¥ (ì´ë¦„ ê·œì¹™ ë³€ê²½)
    # is_bestê°€ Trueì¸ ê²½ìš°: TS_BEST_ì‹œë„ëª…_...png
    # is_bestê°€ Falseì¸ ê²½ìš°: TS_ì‹œë„ëª…_...png (ë³‘í•© ëŒ€ìƒ)
    prefix = 'TS_BEST' if is_best else 'TS'
    file_name = f"{prefix}_{apt_info['ì‹œë„ëª…']}_{apt_info['ì‹œêµ°êµ¬ëª…']}_{apt_info['ë²•ì •ë™']}_{apt_info['ì•„íŒŒíŠ¸']}_{apt_info['ì „ìš©ë©´ì ']:.2f}m2.png".replace('/', '_').replace('.', '_')
    save_path = os.path.join(PLOT_OUTPUT_DIR, file_name)
    plt.savefig(save_path)
    plt.close()  # ë©”ëª¨ë¦¬ í•´ì œ
    return file_name  # ë³‘í•©ì„ ìœ„í•´ íŒŒì¼ ì´ë¦„ ë°˜í™˜


# **********************************************
# ** ì´ë¯¸ì§€ ë³‘í•© í•¨ìˆ˜ (ìš”ì²­í•˜ì‹  í•¨ìˆ˜ êµ¬ì¡°ë¥¼ ë”°ë¦„) **
# **********************************************
def combine_images_to_grid(input_dir, output_filename, except_filename, grid_size=(3, 3)):
    """
    íŠ¹ì • íŒ¨í„´ì˜ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ê·¸ë¦¬ë“œ í˜•íƒœë¡œ í•©ì³ í•˜ë‚˜ì˜ PNG íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    (í•˜ìœ„ 9ê°œ ì•„íŒŒíŠ¸ë¥¼ 3x3 ê·¸ë¦¬ë“œë¡œ í•©ì¹˜ê¸° ìœ„í•´ grid_size=(3, 3)ìœ¼ë¡œ ìˆ˜ì •)
    """
    print("\n--- Starting to combine plot images into a single grid image ---")

    # 1. 'TS'ë¡œ ì‹œì‘í•˜ê³  'except_filename'ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    except_prefix = f"TS_BEST_{except_filename}"  # ë² ìŠ¤íŠ¸ ì•„íŒŒíŠ¸ íŒŒì¼ ì´ë¦„ ì ‘ë‘ì‚¬
    try:
        # PLOT_OUTPUT_DIRì—ì„œ 'TS_'ë¡œ ì‹œì‘í•˜ê³  'TS_BEST_'ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” íŒŒì¼ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        image_files = [
            f for f in os.listdir(input_dir)
            if f.startswith('TS_') and not f.startswith(except_prefix) and f.endswith('.png')
        ]
        image_files.sort()  # íŒŒì¼ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        print(f"Excluding files starting with '{except_prefix}'. {len(image_files)} images found for combination.")
    except FileNotFoundError:
        print(f"Error: Input directory '{input_dir}' not found.")
        return

    if not image_files:
        print("No images found to combine after filtering.")
        return

    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì—´ì–´ ê°œë³„ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    try:
        with Image.open(os.path.join(input_dir, image_files[0])) as img:
            img_width, img_height = img.size
    except Exception as e:
        print(f"Error opening the first image: {e}")
        return

    cols, rows = grid_size
    num_images_to_combine = min(len(image_files), rows * cols)

    # 9ê°œ (3x3)ë§Œ í•©ì¹˜ë¯€ë¡œ, ì „ì²´ ê·¸ë¦¬ë“œ í¬ê¸°ëŠ” 3x3ì— ë§ì¶¥ë‹ˆë‹¤.
    total_width = img_width * cols
    total_height = img_height * rows

    grid_image = Image.new('RGB', (total_width, total_height), 'white')
    print(f"Creating a new {cols}x{rows} grid image with individual size {img_width}x{img_height}...")

    for index, filename in enumerate(image_files):
        if index >= num_images_to_combine:
            break

        row = index // cols
        col = index % cols
        x_offset = col * img_width
        y_offset = row * img_height

        try:
            with Image.open(os.path.join(input_dir, filename)) as img:
                grid_image.paste(img, (x_offset, y_offset))
        except Exception as e:
            print(f"Error processing {filename}: {e}")
            continue

    # 9ê°œì˜ ì´ë¯¸ì§€ë¥¼ í•©ì³¤ìœ¼ë¯€ë¡œ, ìµœì¢… ì´ë¯¸ì§€ë¥¼ ë³´ê¸° ì¢‹ê²Œ ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤.
    # 3x3 ë¹„ìœ¨(42:21)ì— ë§ëŠ” 3000x2100 (14:10) ë˜ëŠ” 3000x1800 (15:9) ë¹„ìœ¨ë¡œ ì¡°ì •
    target_width, target_height = 3000, 1800  # 3x3 ê·¸ë¦¬ë“œì— ì í•©í•œ ê°€ë¡œë¡œ ê¸´ ë¹„ìœ¨
    print(f"Resizing final image to {target_width}x{target_height}...")
    # Image.Resampling.LANCZOS ëŒ€ì‹  Image.LANCZOS ì‚¬ìš© (PIL ë²„ì „ í˜¸í™˜ì„±)
    resized_image = grid_image.resize((target_width, target_height), Image.LANCZOS)

    output_path = os.path.join(input_dir, output_filename)
    resized_image.save(output_path, dpi=(150, 150))
    print(f"--- Successfully combined and resized {num_images_to_combine} images into '{output_path}' ---")

    # ë³‘í•©ì— ì‚¬ìš©ëœ ê°œë³„ íŒŒì¼ ì‚­ì œ
    print("Deleting temporary individual plot files...")
    for filename in image_files[:num_images_to_combine]:
        os.remove(os.path.join(input_dir, filename))
    print("Temporary files deleted.")


# **********************************************
# ** ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹œì‘ **
# **********************************************

# í°íŠ¸ ì„¤ì • ì‹¤í–‰
set_font()

# trained_data ë° results í´ë” ìƒì„±
if not os.path.exists(TRAINED_DATA_DIR):
    os.makedirs(TRAINED_DATA_DIR)
if not os.path.exists(PLOT_OUTPUT_DIR):
    os.makedirs(PLOT_OUTPUT_DIR)

# --- ë°ì´í„° íŒŒì¼ ì„¤ì • ë° ë¡œë“œ ---
data_dir = './preprocessed'
data_csv_path = prepare_csv_from_zip(data_dir, 'KoreaApartDeal_PreProcessed.csv', 'KoreaApartDeal_PreProcessed.zip')

# --- 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ê³µí•™ (ì €ì¥/ë¡œë“œ ë¡œì§) ---
# UniqueIDë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œê°€ ì›ë³¸ ì½”ë“œì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„, ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹œì (SHOULD_RETRAIN == True)ì— ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.

if (os.path.exists(PRELOAD_FILE_PATH) and
        os.path.exists(ENCODERS_FILE_PATH) and
        not SHOULD_RETRAIN):
    # ë¡œë“œ ë¡œì§
    # ... (ìƒëµ: ì´ì „ ì½”ë“œì™€ ë™ì¼)
    print(f"\nì „ì²˜ë¦¬ëœ íŒŒì¼ ë°œê²¬: '{PRELOAD_FILE_PATH}'. íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    try:
        dtype_spec_loaded = {col: 'int' for col in ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸']}
        df = pd.read_csv(PRELOAD_FILE_PATH, dtype=dtype_spec_loaded)
        df['ê±°ë˜ì¼'] = pd.to_datetime(df['ê±°ë˜ì¼'])
        label_encoders = joblib.load(ENCODERS_FILE_PATH)
        base_deal_date = df['ê±°ë˜ì¼'].min()
        print("ì „ì²˜ë¦¬ëœ ë°ì´í„° ë° LabelEncoder ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. ìƒˆë¡œ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        SHOULD_RETRAIN = True
else:
    SHOULD_RETRAIN = True

if SHOULD_RETRAIN:
    print("\nXGBoostë¥¼ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    try:
        dtype_spec = {'ì¸µ': 'object'}
        df = pd.read_csv(data_csv_path, dtype=dtype_spec)
    except Exception as e:
        print(f"file read error: {e}")
        sys.exit(1)

    df['ê±°ë˜ê¸ˆì•¡'] = pd.to_numeric(df['ê±°ë˜ê¸ˆì•¡'], errors='coerce')
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡'], inplace=True)

    core_features = ['ê±°ë˜ì¼', 'ê±´ì¶•ë…„ë„', 'ì „ìš©ë©´ì ', 'ê±°ë˜ê¸ˆì•¡', 'ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸']
    df.dropna(subset=core_features, inplace=True)

    # df['ì¸µ'] = pd.to_numeric(df['ì¸µ'], errors='coerce')
    # df.dropna(subset=['ì¸µ'], inplace=True)
    # df['ì¸µ'] = df['ì¸µ'].astype(int)

    df['ê±°ë˜ì¼'] = pd.to_datetime(df['ê±°ë˜ì¼'], errors='coerce')
    df.dropna(subset=['ê±°ë˜ì¼'], inplace=True)
    df['ê±´ì¶•ë…„ë„'] = df['ê±´ì¶•ë…„ë„'].astype(int)

    df['ê±°ë˜_ë…„'] = df['ê±°ë˜ì¼'].dt.year
    df['ê±°ë˜_ì›”'] = df['ê±°ë˜ì¼'].dt.month
    df['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = df['ê±°ë˜_ë…„'] - df['ê±´ì¶•ë…„ë„']
    base_deal_date = df['ê±°ë˜ì¼'].min()
    df['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (df['ê±°ë˜ì¼'] - base_deal_date).dt.days

    # **UniqueID ìƒì„± (í•„ìˆ˜)**
    # df['UniqueID'] = df.apply(
    #     lambda row: f"{row['ì‹œë„ëª…']}_{row['ì‹œêµ°êµ¬ëª…']}_{row['ë²•ì •ë™']}_{row['ì•„íŒŒíŠ¸']}_{row['ì „ìš©ë©´ì ']:.2f}_{row['ê±´ì¶•ë…„ë„']}}",
    #     axis=1
    # )

    # ìµœì†Œ ê±°ë˜ íšŸìˆ˜ í•„í„°ë§
    apt_counts = df['UniqueID'].value_counts()
    valid_uids = apt_counts[apt_counts >= MIN_TRANSACTION_COUNT].index
    df = df[df['UniqueID'].isin(valid_uids)].copy()
    print(f"ìµœì†Œ {MIN_TRANSACTION_COUNT}íšŒ ì´ìƒ ê±°ë˜ëœ ì•„íŒŒíŠ¸ë¡œ í•„í„°ë§ í›„, ë‚¨ì€ ê±°ë˜ ê¸°ë¡: {len(df)}ê°œ")

    # Label Encoding ìˆ˜í–‰
    cat_features = ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸']
    label_encoders = {}
    for col in cat_features:
        df[col] = df[col].astype(str).fillna('missing')
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

    # ì „ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ ì €ì¥
    try:
        df.to_csv(PRELOAD_FILE_PATH, index=False, encoding='utf-8')
        joblib.dump(label_encoders, ENCODERS_FILE_PATH)
        print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„°ì™€ ì¸ì½”ë”ê°€ '{TRAINED_DATA_DIR}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì „ì²˜ë¦¬ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

features = ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ì „ìš©ë©´ì ',
            'ê±´ì¶•ë…„ë„', 'ê±°ë˜_ë…„', 'ê±°ë˜_ì›”', 'ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜', 'ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜']  # ì¸µì„ ë¹¼ê³  ì˜ˆì¸¡ì— ì‚¬ìš© (UniqueIDì— í¬í•¨ë¨)
target = 'ê±°ë˜ê¸ˆì•¡'

# ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ Xì— í• ë‹¹
X = df[features]
y = df[target]

# --- 4. ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° ---
if os.path.exists(MODEL_FILE_PATH) and not SHOULD_RETRAIN:
    try:
        xgb_model = joblib.load(MODEL_FILE_PATH)
        print("XGBoost ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. ìƒˆë¡œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        SHOULD_RETRAIN = True
else:
    SHOULD_RETRAIN = True

if SHOULD_RETRAIN:
    print("\nëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ìƒˆë¡œìš´ í›ˆë ¨/ì¬í›ˆë ¨)...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    xgb_model = XGBRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=6,
        random_state=42,
        n_jobs=-1
    )
    xgb_model.fit(X_train, y_train)
    print("XGBoost ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ.")
    try:
        joblib.dump(xgb_model, MODEL_FILE_PATH)
        print(f"í›ˆë ¨ëœ ëª¨ë¸ì´ '{MODEL_FILE_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")

# --- 5. ì „êµ­ ëŒ€ìƒ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…‹ ìƒì„± ë° ì˜ˆì¸¡ ---
all_unique_apts = df.drop_duplicates(
    subset=['UniqueID']
).reset_index(drop=True).copy()
print(f"\nì „êµ­ {len(all_unique_apts)}ê°œ ê³ ìœ  ì•„íŒŒíŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì‹œì‘.")

predict_date_2025 = pd.to_datetime('2025-12-01')
predict_date_2030 = pd.to_datetime('2030-12-01')

base_cols = ['UniqueID', 'ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ì „ìš©ë©´ì ', 'ê±´ì¶•ë…„ë„']
buy_X = all_unique_apts[base_cols].copy()
buy_X['ê±°ë˜_ë…„'] = predict_date_2025.year
buy_X['ê±°ë˜_ì›”'] = predict_date_2025.month
buy_X['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = buy_X['ê±°ë˜_ë…„'] - buy_X['ê±´ì¶•ë…„ë„']
buy_X['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (predict_date_2025 - base_deal_date).days

sell_X = all_unique_apts[base_cols].copy()
sell_X['ê±°ë˜_ë…„'] = predict_date_2030.year
sell_X['ê±°ë˜_ì›”'] = predict_date_2030.month
sell_X['ê±´ì¶•_ê²½ê³¼ë…„ìˆ˜'] = sell_X['ê±°ë˜_ë…„'] - sell_X['ê±´ì¶•ë…„ë„']
sell_X['ìµœê·¼_ê±°ë˜ì¼_ì ìˆ˜'] = (predict_date_2030 - base_deal_date).days

buy_X_model = buy_X[features]
sell_X_model = sell_X[features]

print("\nëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì‹œì‘ (ì „êµ­)...")
buy_prices_2025 = xgb_model.predict(buy_X_model)
sell_prices_2030 = xgb_model.predict(sell_X_model)
print("ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ.")

reco_df = all_unique_apts[['UniqueID', 'ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ì „ìš©ë©´ì ', 'ê±´ì¶•ë…„ë„']].copy()
reco_df['ë§¤ì…ì˜ˆìƒê°€_2025_12'] = buy_prices_2025.astype(int)
reco_df['ë§¤ê°ì˜ˆìƒê°€_2030_12'] = sell_prices_2030.astype(int)
reco_df['ì˜ˆìƒ_ìµœëŒ€ì´ìµ'] = reco_df['ë§¤ê°ì˜ˆìƒê°€_2030_12'] - reco_df['ë§¤ì…ì˜ˆìƒê°€_2025_12']

for col in cat_features:
    reco_df[col] = label_encoders[col].inverse_transform(reco_df[col].astype(int))

reco_df = reco_df.sort_values(by='ì˜ˆìƒ_ìµœëŒ€ì´ìµ', ascending=False).reset_index(drop=True)

sido_list = sorted(reco_df['ì‹œë„ëª…'].unique())
sido_map = {i + 1: sido for i, sido in enumerate(sido_list)}

# --- 7. ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ë™ì  í•„í„°ë§ ë° ì¶œë ¥ (ë¬´í•œ ë°˜ë³µ) ---
while True:
    print("\n" + "=" * 50)
    print("ì§€ì—­ ì„ íƒ: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³¼ **ì‹œë„ëª…**ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    print("0: í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("=" * 50)
    for num, sido in sido_map.items():
        print(f"{num}: {sido}")
    print("=" * 50)

    selected_num = None
    try:
        user_input = input("ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0 ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if user_input.strip() == '':
            continue
        selected_num = int(user_input)
    except ValueError:
        print("ìœ íš¨í•œ ìˆ«ì í˜•ì‹ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        continue

    if selected_num == 0:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.")
        sys.exit(0)

    if selected_num not in sido_map:
        print("ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ëª©ë¡ì— ìˆëŠ” ë²ˆí˜¸(1 ì´ìƒ)ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        continue

    selected_sido = sido_map[selected_num]
    filtered_df = reco_df[reco_df['ì‹œë„ëª…'] == selected_sido].copy()

    if filtered_df.empty:
        print(f"\nê²½ê³ : {selected_sido} ì§€ì—­ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        continue

    # 7.3. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥ ì¤€ë¹„
    top_10_apts = filtered_df.head(10).copy()
    best_apt = top_10_apts.iloc[0]

    # ì¶œë ¥ ë‚´ìš© ë¬¸ìì—´ë¡œ êµ¬ì„±
    output_text = "\n" + "=" * 70 + "\n"
    output_text += f"ğŸ  {selected_sido} ìµœëŒ€ ì´ìµ ì•„íŒŒíŠ¸ ì¶”ì²œ ê²°ê³¼ (ë‹¨ìœ„: ë§Œ ì›)\n"
    output_text += "=" * 70 + "\n"
    output_text += f"**ìµœì  ì•„íŒŒíŠ¸:** {best_apt['ì•„íŒŒíŠ¸']} ({best_apt['ì‹œêµ°êµ¬ëª…']} {best_apt['ë²•ì •ë™']})\n"
    output_text += f"**ì „ìš©ë©´ì :** {best_apt['ì „ìš©ë©´ì ']:.2f} mÂ²\n"
    output_text += f"**2025ë…„ 12ì›” ì˜ˆìƒ ë§¤ì…ê°€:** {format_manwon(best_apt['ë§¤ì…ì˜ˆìƒê°€_2025_12'])}\n"
    output_text += f"**2030ë…„ 12ì›” ì˜ˆìƒ ë§¤ê°ê°€:** {format_manwon(best_apt['ë§¤ê°ì˜ˆìƒê°€_2030_12'])}\n"
    output_text += f"**ì˜ˆìƒ ìµœëŒ€ ì´ìµ (5ë…„):** {format_manwon(best_apt['ì˜ˆìƒ_ìµœëŒ€ì´ìµ'])}\n"
    output_text += "=" * 70 + "\n"

    output_text += f"\nìƒìœ„ 10ê°œ ì¶”ì²œ ì•„íŒŒíŠ¸ ëª©ë¡ ({selected_sido}, ì´ìµ ë§Œ ì› ê¸°ì¤€)\n"
    display_cols = ['ì‹œë„ëª…', 'ì‹œêµ°êµ¬ëª…', 'ë²•ì •ë™', 'ì•„íŒŒíŠ¸', 'ì „ìš©ë©´ì ', 'ì˜ˆìƒ_ìµœëŒ€ì´ìµ', 'ë§¤ì…ì˜ˆìƒê°€_2025_12', 'ë§¤ê°ì˜ˆìƒê°€_2030_12']

    top_10_string = top_10_apts[display_cols].to_string(
        index=False,
        formatters={
            'ì˜ˆìƒ_ìµœëŒ€ì´ìµ': '{:,.0f}'.format,
            'ë§¤ì…ì˜ˆìƒê°€_2025_12': '{:,.0f}'.format,
            'ë§¤ê°ì˜ˆìƒê°€_2030_12': '{:,.0f}'.format,
            'ì „ìš©ë©´ì ': '{:.2f}'.format,
        }
    )
    output_text += top_10_string + "\n"
    output_text += "\n" + "=" * 70 + "\n"
    output_text += f"ê²°ê³¼ëŠ” ./{PLOT_OUTPUT_DIR}/{selected_sido}_Apt_Recommendation.txt íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
    output_text += "ìµœì  ì•„íŒŒíŠ¸ëŠ” ê°œë³„ PNG íŒŒì¼ë¡œ, ë‚˜ë¨¸ì§€ 9ê°œëŠ” í•˜ë‚˜ì˜ PNG íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤."
    # ì½˜ì†” ì¶œë ¥
    print(output_text)

    # íŒŒì¼ ì €ì¥
    recommendation_file_name = f"{selected_sido}_Apt_Recommendation.txt"
    recommendation_file_path = os.path.join(PLOT_OUTPUT_DIR, recommendation_file_name)
    try:
        with open(recommendation_file_path, 'w', encoding='utf-8') as f:
            f.write(output_text)
        print(f"ì¶”ì²œ ê²°ê³¼ê°€ '{recommendation_file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê²½ê³ : ì¶”ì²œ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

    # 7.4. ìƒìœ„ 10ê°œ ì•„íŒŒíŠ¸ ì‹œê³„ì—´ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ ë° ë³‘í•© ë¡œì§

    # 1. ìµœëŒ€ ì´ìµ ì•„íŒŒíŠ¸ (top 1) ê°œë³„ ì €ì¥
    print(f"\n[1/2] ìµœì  ì•„íŒŒíŠ¸ ({best_apt['ì•„íŒŒíŠ¸']}) ì‹œê³„ì—´ ì°¨íŠ¸ ê°œë³„ ì €ì¥...")
    plot_apartment_timeseries(
        unique_id=best_apt['UniqueID'],
        original_df=df,
        reco_df=reco_df,
        model=xgb_model,
        base_date=base_deal_date,
        features=features,
        is_best=True  # ê°œë³„ íŒŒì¼ ì €ì¥ í”Œë˜ê·¸
    )
    best_apt_filename_prefix = f"{selected_sido}_{best_apt['ì‹œêµ°êµ¬ëª…']}_{best_apt['ë²•ì •ë™']}_{best_apt['ì•„íŒŒíŠ¸']}"

    # 2. í•˜ìœ„ 9ê°œ ì•„íŒŒíŠ¸ (top 2 ~ 10) ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    print(f"\n[2/2] í•˜ìœ„ 9ê°œ ì•„íŒŒíŠ¸ ì‹œê³„ì—´ ì°¨íŠ¸ ì„ì‹œ íŒŒì¼ ì €ì¥ ë° ë³‘í•© ì‹œì‘...")

    # ì„ì‹œ íŒŒì¼ ëª©ë¡ì„ ì¶”ì í•©ë‹ˆë‹¤. (combine_images_to_grid í•¨ìˆ˜ì—ì„œ ì‚­ì œë  ì˜ˆì •)
    temp_files = []

    for i in range(1, len(top_10_apts)):  # ì¸ë±ìŠ¤ 1ë¶€í„° ì‹œì‘ (2ë²ˆì§¸ ì•„íŒŒíŠ¸ë¶€í„°)
        apt = top_10_apts.iloc[i]
        filename = plot_apartment_timeseries(
            unique_id=apt['UniqueID'],
            original_df=df,
            reco_df=reco_df,
            model=xgb_model,
            base_date=base_deal_date,
            features=features,
            is_best=False  # ë³‘í•©ìš© íŒŒì¼ ì €ì¥ í”Œë˜ê·¸
        )
        if filename:
            temp_files.append(filename)

    # 3. ì„ì‹œ íŒŒì¼ë“¤ì„ ê·¸ë¦¬ë“œë¡œ ë³‘í•©
    if temp_files:
        combine_images_to_grid(
            input_dir=PLOT_OUTPUT_DIR,
            output_filename=f"Combined_Top9_Trends_{selected_sido}.png",
            except_filename=best_apt_filename_prefix,  # TS_BEST_ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ ì œì™¸
            grid_size=(3, 3)  # 3x3 ê·¸ë¦¬ë“œ
        )
    else:
        print("ê²½ê³ : í•˜ìœ„ 9ê°œ ì•„íŒŒíŠ¸ ì¤‘ ì‹œê³„ì—´ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ì´ë¯¸ì§€ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    print("#" * 70)